{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Inetsix \u00b6 A simple blog to share findings and content around network and automation. Related content Personnal Github Arista Validated Design Collection Arista Ansible collection for Cloudvision K8s journey Enough of IT, let\u2019s travel a bit","title":"Inetsix"},{"location":"#inetsix","text":"A simple blog to share findings and content around network and automation. Related content Personnal Github Arista Validated Design Collection Arista Ansible collection for Cloudvision K8s journey Enough of IT, let\u2019s travel a bit","title":"Inetsix"},{"location":"gh-repos/","text":"Github Repositories \u00b6 Personal Repositories \u00b6 Project Description Last activity docker-mkdocs Container to serve mkdocs server 2020-09-15 terraform-ansible-vsrx-demo A demo repository to build a Juniper lab by using terraform and ansible 2020-09-14 ansible-avd Ansible Arista Validated Design 2020-09-14 ansible-cvp Ansible modules for Arista CloudVision 2020-09-04 eos-snmp-extension-vlan-counters Provides an example of how to extend net-snmp to expose hardware counters for vlans 2020-08-31 dotfiles curl -s https://raw.githubusercontent.com/titom73/dotfiles/master/.zshrc-inetsix/tools/install.sh bash .devcontainer Repository to manage VScode devcontainer function 2020-08-26 cvprac None 2020-07-25 avd-docker-in-docker AVD image with docker-ce-cli for docker-in-docker 2020-07-25 ansible-avd-cloudvision-demo A demo to run Arista Validated Design with CloudVision deployment - From Powerup to EVPN Fabric 2020-07-08 arista_Check This software allows checking some parameters and create some MAPs. 2020-07-07 k8s-journey Kubernetes resources for lab 2020-06-24 docker-avd-base Docker image to run all Arista AVD activities 2020-06-24 ansible-arista-module-howto Some howto documentation and content to play with Ansible modules for Arista 2020-06-19 spaceship-prompt :rocket::star: A Zsh prompt for Astronauts 2020-05-17 selinux Pure-python selinux shim module for use in virtualenvs 2020-05-02 BgpEvpnCoreMonitor An EOS SDK Agent that Monitors the Status of BGP EVPN peers, and enables/disables ESI interfaces based on the status of the BGP EVPN Peers 2020-04-30 cvp-generic-configlet-builder A generic configlet builder to render J2 templates stored in CVP with configlet and based on JSON data configlet. 2020-02-13 python-excel-serializer Python script to extract Excel data to tranform in any text file by using Jinja2 templates 2019-12-28 contrail-python-demo A Python demo to manage Contrail with scripts 2019-08-22 ansible-junos-ztp Sample project that use Ansible to automate setup and management of a ZTP server to provision Junos devices with software and startup configuration. 2019-06-29 junos-py-demo Demo Script to demonstrate ability to automate Junos with Python 2019-06-08 junoscriptorium A repository for scripts and script libraries 2017-04-11 Inetsix Repositories \u00b6 Project Description Last activity arista-cvp-scripts Some scripts to interact with CloudVision based on cvprac 2020-07-16 molecule-collection-actions Provide a simple action to execute ansible molecule with collection structure 2020-07-13 blog Content for inetsix blog 2020-06-24 docker-sshd Small Alpine Linux container to expose SSH Server 2020-05-18 docker-gitlab-runners Repository to deploy gitlab-runners using docker-compose 2020-05-18 docker-ubuntu-vps Docker Image to deploy Ubuntu VPS 2020-05-18 docker-ansible Docker environment to play with ansible with Junos and AWS components 2020-05-18 avd-for-compose-kubernetes-demo AVD Content to support Docker Compose & Kubernetes POC integration 2020-05-18 demo-avd-compose-k8s A demo to automate AVD exection with docker-compose and Kubernetes 2020-05-18 inetsix-python-lib Python library with generic helpers 2020-05-18 ansible-cvp-configlet-deployment Demo content to demonstrate Ansible + CVP integration for configlet deployment using CI/CD mechanism 2020-04-25 inetsix-config-builder Provide script to massively generate configuration based on Jinja2 and YAML 2020-04-25 inetsix-image-sorter Script to sort JPG images based on their EXIF fields 2017-02-17","title":"Github Repositories"},{"location":"gh-repos/#github-repositories","text":"","title":"Github Repositories"},{"location":"gh-repos/#personal-repositories","text":"Project Description Last activity docker-mkdocs Container to serve mkdocs server 2020-09-15 terraform-ansible-vsrx-demo A demo repository to build a Juniper lab by using terraform and ansible 2020-09-14 ansible-avd Ansible Arista Validated Design 2020-09-14 ansible-cvp Ansible modules for Arista CloudVision 2020-09-04 eos-snmp-extension-vlan-counters Provides an example of how to extend net-snmp to expose hardware counters for vlans 2020-08-31 dotfiles curl -s https://raw.githubusercontent.com/titom73/dotfiles/master/.zshrc-inetsix/tools/install.sh bash .devcontainer Repository to manage VScode devcontainer function 2020-08-26 cvprac None 2020-07-25 avd-docker-in-docker AVD image with docker-ce-cli for docker-in-docker 2020-07-25 ansible-avd-cloudvision-demo A demo to run Arista Validated Design with CloudVision deployment - From Powerup to EVPN Fabric 2020-07-08 arista_Check This software allows checking some parameters and create some MAPs. 2020-07-07 k8s-journey Kubernetes resources for lab 2020-06-24 docker-avd-base Docker image to run all Arista AVD activities 2020-06-24 ansible-arista-module-howto Some howto documentation and content to play with Ansible modules for Arista 2020-06-19 spaceship-prompt :rocket::star: A Zsh prompt for Astronauts 2020-05-17 selinux Pure-python selinux shim module for use in virtualenvs 2020-05-02 BgpEvpnCoreMonitor An EOS SDK Agent that Monitors the Status of BGP EVPN peers, and enables/disables ESI interfaces based on the status of the BGP EVPN Peers 2020-04-30 cvp-generic-configlet-builder A generic configlet builder to render J2 templates stored in CVP with configlet and based on JSON data configlet. 2020-02-13 python-excel-serializer Python script to extract Excel data to tranform in any text file by using Jinja2 templates 2019-12-28 contrail-python-demo A Python demo to manage Contrail with scripts 2019-08-22 ansible-junos-ztp Sample project that use Ansible to automate setup and management of a ZTP server to provision Junos devices with software and startup configuration. 2019-06-29 junos-py-demo Demo Script to demonstrate ability to automate Junos with Python 2019-06-08 junoscriptorium A repository for scripts and script libraries 2017-04-11","title":"Personal Repositories"},{"location":"gh-repos/#inetsix-repositories","text":"Project Description Last activity arista-cvp-scripts Some scripts to interact with CloudVision based on cvprac 2020-07-16 molecule-collection-actions Provide a simple action to execute ansible molecule with collection structure 2020-07-13 blog Content for inetsix blog 2020-06-24 docker-sshd Small Alpine Linux container to expose SSH Server 2020-05-18 docker-gitlab-runners Repository to deploy gitlab-runners using docker-compose 2020-05-18 docker-ubuntu-vps Docker Image to deploy Ubuntu VPS 2020-05-18 docker-ansible Docker environment to play with ansible with Junos and AWS components 2020-05-18 avd-for-compose-kubernetes-demo AVD Content to support Docker Compose & Kubernetes POC integration 2020-05-18 demo-avd-compose-k8s A demo to automate AVD exection with docker-compose and Kubernetes 2020-05-18 inetsix-python-lib Python library with generic helpers 2020-05-18 ansible-cvp-configlet-deployment Demo content to demonstrate Ansible + CVP integration for configlet deployment using CI/CD mechanism 2020-04-25 inetsix-config-builder Provide script to massively generate configuration based on Jinja2 and YAML 2020-04-25 inetsix-image-sorter Script to sort JPG images based on their EXIF fields 2017-02-17","title":"Inetsix Repositories"},{"location":"yaml-jinja/","text":"YAML Structures in Jinja2 \u00b6 Data structure to use YAML with JINJA2 templates. YAML keeps data stored as a map containing keys and values associated to those keys. Basic \u00b6 Generic key allocation \u00b6 YAML input data structure --- time_zone : Europe/Paris hostname : myDevice comment : \u201cThis device is a fake one\u201d JINJA2 to consume YAML data structure system { {% if time_zone is defined %} time-zone {{ time_zone }}; {% endif %} Structured key allocation \u00b6 YAML will consider lines prefixed with more spaces than parent key are contained inside it --- routing_policy : communities : myCommunity : target:99:1 tenant2 : target:99:2 Data structure similar to this YAML representation is : routing_policy = { \"communities\" : { \"myCommunity\" : \"target:99:1\" , \"tenant2\" : \"target:99:2\" } } List Management \u00b6 YAML List \u00b6 Assuming following data structure in YAML --- ntp_servers : - 8.8.8.8 - 4.4.4.4 Access data from JINJA2 can be done with the following code: {% for ntp_server in ntp_servers %} server {{ ntp_server }}; {% endfor %} Data structure similar to this YAML representation is : ntp_servers = [ '8.8.8.8' , '4.4.4.4' ] YAML Dictionary \u00b6 Assuming following data structure in YAML --- vlans : 10 : name1 20 : name2 30 : name3 Access data from JINJA2 can be done with the following code: {% for vlan_id, name in vlans.items() %} vlan {{ vlan_id }} name {{ name }} {% endfor %} Data structure similar to this YAML representation is : vlans = { 10 : \"name1\" , 20 : \"name2\" , 30 : \"name3\" } YAML Maps \u00b6 Assuming following data structure in YAML --- interfaces : - name : \"ge-0/0/0\" descr : \"Blah\" - name : \"ge-0/0/1\" descr : \"comment\" Access data from JINJA2 can be done with the following code: {% for interface in interfaces %} {{interface.name}} { description {{interface.descr}}; vlan-tagging; {% endfor %} Data structure similar to this YAML representation is : interfaces = [{ \"name\" : \"ge-0/0/0\" , \"descr\" : \"Blah\" }, { \"name\" : \"ge-0/0/1\" , \"descr\" : \"comment\" }] Advanced Jinja2 syntax \u00b6 Update variable in a Loop \u00b6 Syntax below allows user to update value of a variable within a loop and access to it after in a different jinja2 block: {% set ns = namespace (dev_state = \"disable\") %} {% for portname, portlist in topo[inventory_hostname].iteritems() %} {% if portlist.state == \"enable\" %} {% set ns.dev_state = \"enable\" %} {% endif %} {% endfor %} status: {{ns.dev_state|default(\"enable\")}} Use IPaddr within Jinja2 template \u00b6 Assuming following yaml definition: id : 10 underlay : networks : loopbacks : 10.0.0.0/16 Jinja gives option to build IP address within loopback network with following syntax where id is the idth in the network: loopback_ip: {{ underlay.networks.loopbacks | ipaddr(id) | ipaddr('address') }} Another example: {{ tenant.interconnect_prefix | ipaddr(6)}} combined with interconnect_prefix: 172.25.10.16/28 resulst in 172.25.10.22 Manage Jinja2 rendering indentation \u00b6 Jinja2 can manage whitespace and tabular indentation with lstrip_blocks and trim_blocks options: trim_blocks : If this is set to True the first newline after a block is removed (block, not variable tag!). Defaults to False . lstrip_blocks : If this is set to True leading spaces and tabs are stripped from the start of a line to a block. Defaults to False . To manage these options, just put this line in your template: #jinja2: lstrip_blocks: \"True (or False)\", trim_blocks: \"True (or False)\" ... Example Using this template: {% for host in groups['webservers'] %} {% if inventory_hostname in hostvars[host]['ansible_fqdn'] %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['inventory_hostname'] }} MYSELF {% else %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ hostvars[host]['ansible_fqdn'] }} jcs-server{{ loop.index }} {{ hostvars[host]['inventory_hostname'] }} {% endif %} {% endfor %} lstrip_block=False Rendering is similar to : 172.16.25.1 spine1 172.16.25.3 spine2 172.16.25.4 spine3 lstrip_block=true Rendering should be: 172.16.25.1 spine1 172.16.25.3 spine2 172.16.25.4 spine3 Loop management \u00b6 Jinja2 has built-in option to manage loop information: Variable Description loop.index The current iteration of the loop. (1 indexed) loop.index0 The current iteration of the loop. (0 indexed) loop.revindex The number of iterations from the end of the loop (1 indexed) loop.revindex0 The number of iterations from the end of the loop (0 indexed) loop.first True if first iteration. loop.last True if last iteration. loop.length The number of items in the sequence. loop.cycle A helper function to cycle between a list of sequences. See the explanation below. loop.depth Indicates how deep in deep in a recursive loop the rendering currently is. Starts at level 1 loop.depth0 Indicates how deep in deep in a recursive loop the rendering currently is. Starts at level 0","title":"YAML Structures in Jinja2"},{"location":"yaml-jinja/#yaml-structures-in-jinja2","text":"Data structure to use YAML with JINJA2 templates. YAML keeps data stored as a map containing keys and values associated to those keys.","title":"YAML Structures in Jinja2"},{"location":"yaml-jinja/#basic","text":"","title":"Basic"},{"location":"yaml-jinja/#generic-key-allocation","text":"YAML input data structure --- time_zone : Europe/Paris hostname : myDevice comment : \u201cThis device is a fake one\u201d JINJA2 to consume YAML data structure system { {% if time_zone is defined %} time-zone {{ time_zone }}; {% endif %}","title":"Generic key allocation"},{"location":"yaml-jinja/#structured-key-allocation","text":"YAML will consider lines prefixed with more spaces than parent key are contained inside it --- routing_policy : communities : myCommunity : target:99:1 tenant2 : target:99:2 Data structure similar to this YAML representation is : routing_policy = { \"communities\" : { \"myCommunity\" : \"target:99:1\" , \"tenant2\" : \"target:99:2\" } }","title":"Structured key allocation"},{"location":"yaml-jinja/#list-management","text":"","title":"List Management"},{"location":"yaml-jinja/#yaml-list","text":"Assuming following data structure in YAML --- ntp_servers : - 8.8.8.8 - 4.4.4.4 Access data from JINJA2 can be done with the following code: {% for ntp_server in ntp_servers %} server {{ ntp_server }}; {% endfor %} Data structure similar to this YAML representation is : ntp_servers = [ '8.8.8.8' , '4.4.4.4' ]","title":"YAML List"},{"location":"yaml-jinja/#yaml-dictionary","text":"Assuming following data structure in YAML --- vlans : 10 : name1 20 : name2 30 : name3 Access data from JINJA2 can be done with the following code: {% for vlan_id, name in vlans.items() %} vlan {{ vlan_id }} name {{ name }} {% endfor %} Data structure similar to this YAML representation is : vlans = { 10 : \"name1\" , 20 : \"name2\" , 30 : \"name3\" }","title":"YAML Dictionary"},{"location":"yaml-jinja/#yaml-maps","text":"Assuming following data structure in YAML --- interfaces : - name : \"ge-0/0/0\" descr : \"Blah\" - name : \"ge-0/0/1\" descr : \"comment\" Access data from JINJA2 can be done with the following code: {% for interface in interfaces %} {{interface.name}} { description {{interface.descr}}; vlan-tagging; {% endfor %} Data structure similar to this YAML representation is : interfaces = [{ \"name\" : \"ge-0/0/0\" , \"descr\" : \"Blah\" }, { \"name\" : \"ge-0/0/1\" , \"descr\" : \"comment\" }]","title":"YAML Maps"},{"location":"yaml-jinja/#advanced-jinja2-syntax","text":"","title":"Advanced Jinja2 syntax"},{"location":"yaml-jinja/#update-variable-in-a-loop","text":"Syntax below allows user to update value of a variable within a loop and access to it after in a different jinja2 block: {% set ns = namespace (dev_state = \"disable\") %} {% for portname, portlist in topo[inventory_hostname].iteritems() %} {% if portlist.state == \"enable\" %} {% set ns.dev_state = \"enable\" %} {% endif %} {% endfor %} status: {{ns.dev_state|default(\"enable\")}}","title":"Update variable in a Loop"},{"location":"yaml-jinja/#use-ipaddr-within-jinja2-template","text":"Assuming following yaml definition: id : 10 underlay : networks : loopbacks : 10.0.0.0/16 Jinja gives option to build IP address within loopback network with following syntax where id is the idth in the network: loopback_ip: {{ underlay.networks.loopbacks | ipaddr(id) | ipaddr('address') }} Another example: {{ tenant.interconnect_prefix | ipaddr(6)}} combined with interconnect_prefix: 172.25.10.16/28 resulst in 172.25.10.22","title":"Use IPaddr within Jinja2 template"},{"location":"yaml-jinja/#manage-jinja2-rendering-indentation","text":"Jinja2 can manage whitespace and tabular indentation with lstrip_blocks and trim_blocks options: trim_blocks : If this is set to True the first newline after a block is removed (block, not variable tag!). Defaults to False . lstrip_blocks : If this is set to True leading spaces and tabs are stripped from the start of a line to a block. Defaults to False . To manage these options, just put this line in your template: #jinja2: lstrip_blocks: \"True (or False)\", trim_blocks: \"True (or False)\" ... Example Using this template: {% for host in groups['webservers'] %} {% if inventory_hostname in hostvars[host]['ansible_fqdn'] %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['inventory_hostname'] }} MYSELF {% else %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ hostvars[host]['ansible_fqdn'] }} jcs-server{{ loop.index }} {{ hostvars[host]['inventory_hostname'] }} {% endif %} {% endfor %} lstrip_block=False Rendering is similar to : 172.16.25.1 spine1 172.16.25.3 spine2 172.16.25.4 spine3 lstrip_block=true Rendering should be: 172.16.25.1 spine1 172.16.25.3 spine2 172.16.25.4 spine3","title":"Manage Jinja2 rendering indentation"},{"location":"yaml-jinja/#loop-management","text":"Jinja2 has built-in option to manage loop information: Variable Description loop.index The current iteration of the loop. (1 indexed) loop.index0 The current iteration of the loop. (0 indexed) loop.revindex The number of iterations from the end of the loop (1 indexed) loop.revindex0 The number of iterations from the end of the loop (0 indexed) loop.first True if first iteration. loop.last True if last iteration. loop.length The number of items in the sequence. loop.cycle A helper function to cycle between a list of sequences. See the explanation below. loop.depth Indicates how deep in deep in a recursive loop the rendering currently is. Starts at level 1 loop.depth0 Indicates how deep in deep in a recursive loop the rendering currently is. Starts at level 0","title":"Loop management"},{"location":"Arista%20AVD/ansible-avd-config/","text":"Arista AVD Configuration overview \u00b6 Overview \u00b6 Arista Networks supports Ansible for managing devices running the EOS operating system natively through eapi or CloudVision Portal (CVP). This collection includes a set of ansible roles and modules to help kick-start your automation with Arista. The various roles and templates provided are designed to be customized and extended to your needs! In this post, we will go through all configuration steps to generate EVPN/VXLAN configuration for EOS devices. Because some files might be very verbose, we demo repository is available to get complete content of all files: inetsix/demo-avd-evpn-eve-ng . Idea is not to cover all EVPN/VXLAN feaures, but go through AVD to see how to implement. You can organize your work in many different way, but a structure I find useful is something like this: A folder for all your inventories with one sub-folder per inventory. An inventory folder contains all your variables for a given environment like host_vars , group_vars , inventory.yml A folder to store all playbooks. So it is easy to reuse playbooks whatever the inventory is (if you use a coherent syntax) ansible.cfg at the root of this repository $ tree -L 3 -d . \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 inetsix-cvp \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2502 \u2514\u2500\u2500 media \u2502 \u2514\u2500\u2500 inetsix-eapi \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 medias \u2514\u2500\u2500 playbooks From there, you can run ansible playbooks with following syntax (as an example) : $ ansible-playbook playbooks/my-playbook -i inventories/inetsix-eapi To support this post, we will use the following topology running on EVE-NG server: Inventory file. \u00b6 In our inventory, let\u2019s list our devices: AVD2_FABRIC represents complete fabric topology we are going to configure with AVD. AVD2_SPINES is a group where all spine devices belongs. AVD2_L3LEAFS : is a group to locate all VTEP devices (LEAFs running EVPN/VXLAN). Part of this group, we create one sub-group for every LEAF (single or MLAG) as highlighted below. AVD_LEAF1 represent LEAF for POD01 and is for an MLAG pair. AVD_LEAF3 represent a single LEAF outside of MLAG. AVD2_L2LEAFS represent Aggregation layer in our current design. This layer is optional, but in our design it is in place. It works like for AVD2_L3LEAFS group. AVD2_TENANTS_NETWORKS is a special group housing 2 groups already configured: AVD2_L{2|3}LEAFS . This group will configure VNI/VLAN across the fabric, so we want to make leafs part of the configuration. AVD2_SERVERS as similar behavior to previous group. Its goal is to configure downlinks to compute nodes. # vim inventories/inetsix-eapi/inventory.yml --- AVD2 : children : AVD2_FABRIC : children : AVD2_SPINES : hosts : AVD2-SPINE1 : ansible_port : 8001 [ ... output truncated ... ] AVD2_L3LEAFS : children : AVD2_LEAF1 : hosts : AVD2-LEAF1A : ansible_port : 8011 AVD2-LEAF1B : ansible_port : 8012 AVD2_LEAF3 : hosts : AVD2-LEAF3A : ansible_port : 8017 [ ... output truncated ... ] AVD2_L2LEAFS : children : AVD2_L2LEAF1 : hosts : AVD2-AGG01 : ansible_port : 8021 [ ... output truncated ... ] AVD2_TENANTS_NETWORKS : children : AVD2_L3LEAFS : AVD2_L2LEAFS : AVD2_SERVERS : children : AVD2_L3LEAFS : AVD2_L2LEAFS : In this file, you can also define management address of all devices. Because in this lab we use Jumphost, we will configure this address in the next section, but at least we configured on which port we will connect to eAPI using ansible_port variable. AVD Variables \u00b6 Based on inventory we did in the previous section, it is time to create group_vars . Generic Fabric Information \u00b6 All the documentation is available here, but below is a short example. All this information will be configured on all devices. # vim inventories/inetsix-eapi/group_vars/AVD2.yml --- # local users local_users : admin : privilege : 15 role : network-admin sha512_password : \"...\" demo : privilege : 15 role : network-admin sha512_password : \"...\" # OOB Management network default gateway. mgmt_gateway : 10.73.254.253 mgmt_destination_networks : - 10.255.2.0/24 - 10.255.3.0/24 - 0.0.0.0/0 # dns servers. name_servers : - 1.1.1.1 - 8.8.8.8 # NTP Servers IP or DNS name, first NTP server will be prefered, and sourced from Managment VRF ntp_servers : - uk.pool.ntp.org - fr.pool.ntp.org Configure Fabric topology \u00b6 Fabric topology is configured in inventories/inetsix-eapi/group_vars/AVD2_FABRIC.yml which is file that covers AVD2_FABRIC group we defined in inventory . This file contains all the base information to create initial configuration: You can also refer to Arista Validated Design documentation to get a description of every single option available. Subnet to use for underlay, loopback and vtep: # Point to Point Network Summary range, assigned as /31 for each # uplink interfaces # Assign range larger then total [spines * total potential leafs * 2] underlay_p2p_network_summary : 172.31.255.0/24 # IP address range for evpn loopback for all switches in fabric, # assigned as /32s # Assign range larger then total spines + total leafs switches overlay_loopback_network_summary : 192.168.255.0/24 # VTEP VXLAN Tunnel source loopback IP for leaf switches, assigned in /32s # Assign range larger then total leaf switches vtep_loopback_network_summary : 192.168.254.0/24 MLAG IP information # mlag pair IP assignment - assign blocks - Assign range larger then # total spines + total leafs switches mlag_ips : leaf_peer_l3 : 10.255.251.0/24 mlag_peer : 10.255.252.0/24 Then, you have to describe devices for each role. Don\u2019t forget to set management IP here. Spine devices # Spine Switches spine : platform : vEOS-LAB bgp_as : 65001 # defines the range of acceptable remote ASNs from leaf switches leaf_as_range : 65101-65132 nodes : AVD2-SPINE1 : id : 1 mgmt_ip : 10.73.254.1/24 AVD2-SPINE2 : id : 2 mgmt_ip : 10.73.254.2/24 VTEP or L3LEAF devices l3leaf : defaults : virtual_router_mac_address : 00:1c:73:00:dc:01 platform : vEOS-LAB bgp_as : 65100 spines : [ AVD2-SPINE1 , AVD2-SPINE2 ] uplink_to_spine_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] [ ... output truncated ... ] node_groups : AVD2_LEAF1 : bgp_as : 65101 nodes : AVD2-LEAF1A : id : 1 mgmt_ip : 10.73.254.11/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet1 , Ethernet1 ] AVD2-LEAF1B : id : 2 mgmt_ip : 10.73.254.12/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet2 , Ethernet2 ] [ ... output truncated ... ] Aggregation switches: Configuration process is similar to L3LEAFS. Complete documentation of all available variables is available in Arista Validated Design documentation . You can also look at variables part of the demo repo we are using for this post. As you can see here, if you want to change name in your inventory, you have to edit filename in group_vars to reflect that change as well as content of this fabric file. Configure device type \u00b6 In each variable file related to a type of devices, we have to instruct AVD what is the role of our devices. --- type : spine # Must be either spine|l3leaf|l2leaf Configure VNI/VLAN across the Fabric. \u00b6 AVD supports mechanism to create VLANs and VNIs and enable traffic forwarding in your overlay. In current version ( v1.0.2 ), only following design listed below are supported: L2 VLANs Symetric IRB model Model defines a set of tenants (user\u2019s defined) where you can configure VRF or l2vlans or a mix of them. Let\u2019s take a look at how we configure such services. All these configurations shall be configured in file AVD2_TENANTS_NETWORKS.yml L2 Services \u00b6 Configure a pure L2 service using EVPN route type 2 only: --- tenants : # Tenant B Specific Information - Pure L2 tenant Tenant_B : mac_vrf_vni_base : 20000 l2vlans : 201 : name : 'B-ELAN-201' tags : [ DC1 ] Tag option allows to configure VLAN only on a subset of the fabric: all devices with this tag will be configured with this vlan. To configure device TAGS and TENANTS options, go to Arista Validated Design documentation In this configuration, VLAN will be created with a tag of 201 and its attached VNI will be configured with 20201 AVD2-LEAF1A#show vlan 201 VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 201 B-ELAN-201 active Po3, Po5, Vx1 AVD2-LEAF1A#show bgp evpn vni 20201 BGP routing table information for VRF default Router identifier 192.168.255.3, local AS number 65101 Route status codes: s - suppressed, * - valid, > - active, # - not installed, E - ECMP head, e - ECMP S - Stale, c - Contributing to ECMP, b - backup % - Pending BGP convergence Origin codes: i - IGP, e - EGP, ? - incomplete AS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop Network Next Hop Metric LocPref Weight Path * > RD: 192.168.255.3:20201 mac-ip 20201 5001.0011.0000 - - - 0 i * > RD: 192.168.255.3:20201 imet 20201 192.168.254.3 - - - 0 i * >Ec RD: 192.168.255.5:20201 imet 20201 192.168.254.5 192.168.254.5 - 100 0 65001 65102 i [... output truncated ...] Symetric IRB model \u00b6 Configure IRB symetric model, use following structure: tenants : # Tenant A Specific Information - VRFs / VLANs Tenant_A : mac_vrf_vni_base : 10000 vrfs : TENANT_A_PROJECT01 : vrf_vni : 11 svis : 110 : name : 'PR01-DMZ' tags : [ DC1 ] enabled : true ip_address_virtual : 10.1.10.254/24 111 : name : 'PR01-TRUST' tags : [ POD02 ] enabled : true ip_address_virtual : 10.1.11.254/24 TENANT_A_PROJECT02 : vrf_vni : 12 vtep_diagnostic : loopback : 100 loopback_ip_range : 10.1.255.0/24 svis : 112 : name : 'PR02-DMZ-GREEN' tags : [ POD01 ] enabled : true ip_address_virtual : 10.1.12.254/24 Example will create 2 VRFs : TENANT_A_PROJECT01 TENANT_A_PROJECT02 In TENANT_A_PROJECT01 , 2 subnets are created and deployed on devices matching TAGS DC1 or POD02 : 10.1.10.0/24 with vlan 110 and vni 10110 10.1.11.0/24 with vlan 111 and vni 10111 In case you deployed this VRF on a MLAG VTEP, an additional vlan is created to allow L3 synchronization within VRF. This vlan is automatically generated with this algorithm: {{ mlag_ibgp_peering_vrfs.base_vlan + (tenants[tenant].vrfs[vrf].vrf_vni - 1) }} In addition to that, each EOS devices will allocate a dynamic VLAN per VRF to support L3 VNI AVD2-LEAF1A#show vlan VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 1 default active Et6, Et7, Et8, PEt6, PEt7, PEt8 110 PR01-DMZ active Cpu, Po3, Po5, Vx1 112 PR02-DMZ-ORANGE active Cpu, Po3, Vx1 201 B-ELAN-201 active Po3, Po5, Vx1 1008* VLAN1008 active Cpu, Po3, Vx1 1009* VLAN1009 active Cpu, Po3, Vx1 3010 MLAG_iBGP_TENANT_A_PROJECT01 active Cpu, Po3 3011 MLAG_iBGP_TENANT_A_PROJECT02 active Cpu, Po3 4093 LEAF_PEER_L3 active Cpu, Po3 4094 MLAG_PEER active Cpu, Po3 * indicates a Dynamic VLAN AVD2-LEAF1A#show vxlan vni VNI to VLAN Mapping for Vxlan1 VNI VLAN Source Interface 802.1Q Tag ----------- ----------- ------------ ------------------- ---------- 11 1008* evpn Vxlan1 1008 12 1009* evpn Vxlan1 1009 10110 110 static Port-Channel5 110 Vxlan1 110 10112 112 static Vxlan1 112 20201 201 static Port-Channel5 201 Vxlan1 201 In TENANT_A_PROJECT02 , we can also see an optional feature named vtep_diagnostic . This option allows you to create a loopback in this VRF and do some connectivity test. Configure downlinks \u00b6 As we have configured L3LS fabric, EVPN/VXLAN overlay, services, it is now time to configure ports to connect servers. Ports should be configured in AVD2_SERVERS.yml . You first have to configure port profile. it is basically a description of how the port will be configured ( access or trunk ) and which set of vlan(s) will be configured --- port_profiles : TENANT_A_B : mode : trunk vlans : \"110-111,201\" A-PR01-DMZ : mode : access vlans : \"110\" This section uses vlan-id so all of these entries must be configured in TENANTS file Then, create port mapping on a per server. Single home server \u00b6 If server is connected to only one leaf to the fabric, following template can be used servers : A-PR01-DMZ-POD01 : # Server name rack : POD01 # Informational RACK adapters : - type : nic server_ports : [ Eth0 ] # Server port to connect switch_ports : [ Ethernet3 ] # Switch port to connect server switches : [ DC1-AGG01 ] # Switch to connect server profile : A-PR01-DMZ # Port profile to apply Whereas most of the information are purely optional as not used by AVD, the last 3 entries are required: switch_ports : Will be used to configure correct port on the switch. switches : Must be switch name defined in your inventory. profile : Profile created previously. Server connected to MLAG \u00b6 In case of connection to MLAG, data structure is the same and only difference is we need to add information about Port-Channel to configure. servers : DCI_RTR01 : rack : DCI adapters : - server_ports : [ Eth1 , Eth2 ] switch_ports : [ Ethernet5 , Ethernet5 ] switches : [ SITE01-BL01A , SITE01-BL01B ] profile : A-PR01-DMZ port_channel : state : present description : PortChannel5 mode : active Configure tasks for AVD \u00b6 We will now configure AVD tasks you may use in your playbook: Create directory structure \u00b6 AVD comes with a role to generate your folder structure . tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders vars : fabric_dir_name : '{{fabric_name}}' Transform EVPN datamodel to device data model \u00b6 AVD provides role eos_l3ls_evpn role to generate intend YAML device configuration: tasks : - name : generate intend variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn Generate device configuration and documentation \u00b6 After device data have been generated, AVD can build EOS configuration as well as documentation in markdown format. tasks : - name : generate device intended config and documention tags : [ build ] import_role : name : eos_cli_config_gen","title":"Arista AVD Configuration overview"},{"location":"Arista%20AVD/ansible-avd-config/#arista-avd-configuration-overview","text":"","title":"Arista AVD Configuration overview"},{"location":"Arista%20AVD/ansible-avd-config/#overview","text":"Arista Networks supports Ansible for managing devices running the EOS operating system natively through eapi or CloudVision Portal (CVP). This collection includes a set of ansible roles and modules to help kick-start your automation with Arista. The various roles and templates provided are designed to be customized and extended to your needs! In this post, we will go through all configuration steps to generate EVPN/VXLAN configuration for EOS devices. Because some files might be very verbose, we demo repository is available to get complete content of all files: inetsix/demo-avd-evpn-eve-ng . Idea is not to cover all EVPN/VXLAN feaures, but go through AVD to see how to implement. You can organize your work in many different way, but a structure I find useful is something like this: A folder for all your inventories with one sub-folder per inventory. An inventory folder contains all your variables for a given environment like host_vars , group_vars , inventory.yml A folder to store all playbooks. So it is easy to reuse playbooks whatever the inventory is (if you use a coherent syntax) ansible.cfg at the root of this repository $ tree -L 3 -d . \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 inetsix-cvp \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2502 \u2514\u2500\u2500 media \u2502 \u2514\u2500\u2500 inetsix-eapi \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 medias \u2514\u2500\u2500 playbooks From there, you can run ansible playbooks with following syntax (as an example) : $ ansible-playbook playbooks/my-playbook -i inventories/inetsix-eapi To support this post, we will use the following topology running on EVE-NG server:","title":"Overview"},{"location":"Arista%20AVD/ansible-avd-config/#inventory-file","text":"In our inventory, let\u2019s list our devices: AVD2_FABRIC represents complete fabric topology we are going to configure with AVD. AVD2_SPINES is a group where all spine devices belongs. AVD2_L3LEAFS : is a group to locate all VTEP devices (LEAFs running EVPN/VXLAN). Part of this group, we create one sub-group for every LEAF (single or MLAG) as highlighted below. AVD_LEAF1 represent LEAF for POD01 and is for an MLAG pair. AVD_LEAF3 represent a single LEAF outside of MLAG. AVD2_L2LEAFS represent Aggregation layer in our current design. This layer is optional, but in our design it is in place. It works like for AVD2_L3LEAFS group. AVD2_TENANTS_NETWORKS is a special group housing 2 groups already configured: AVD2_L{2|3}LEAFS . This group will configure VNI/VLAN across the fabric, so we want to make leafs part of the configuration. AVD2_SERVERS as similar behavior to previous group. Its goal is to configure downlinks to compute nodes. # vim inventories/inetsix-eapi/inventory.yml --- AVD2 : children : AVD2_FABRIC : children : AVD2_SPINES : hosts : AVD2-SPINE1 : ansible_port : 8001 [ ... output truncated ... ] AVD2_L3LEAFS : children : AVD2_LEAF1 : hosts : AVD2-LEAF1A : ansible_port : 8011 AVD2-LEAF1B : ansible_port : 8012 AVD2_LEAF3 : hosts : AVD2-LEAF3A : ansible_port : 8017 [ ... output truncated ... ] AVD2_L2LEAFS : children : AVD2_L2LEAF1 : hosts : AVD2-AGG01 : ansible_port : 8021 [ ... output truncated ... ] AVD2_TENANTS_NETWORKS : children : AVD2_L3LEAFS : AVD2_L2LEAFS : AVD2_SERVERS : children : AVD2_L3LEAFS : AVD2_L2LEAFS : In this file, you can also define management address of all devices. Because in this lab we use Jumphost, we will configure this address in the next section, but at least we configured on which port we will connect to eAPI using ansible_port variable.","title":"Inventory file."},{"location":"Arista%20AVD/ansible-avd-config/#avd-variables","text":"Based on inventory we did in the previous section, it is time to create group_vars .","title":"AVD Variables"},{"location":"Arista%20AVD/ansible-avd-config/#generic-fabric-information","text":"All the documentation is available here, but below is a short example. All this information will be configured on all devices. # vim inventories/inetsix-eapi/group_vars/AVD2.yml --- # local users local_users : admin : privilege : 15 role : network-admin sha512_password : \"...\" demo : privilege : 15 role : network-admin sha512_password : \"...\" # OOB Management network default gateway. mgmt_gateway : 10.73.254.253 mgmt_destination_networks : - 10.255.2.0/24 - 10.255.3.0/24 - 0.0.0.0/0 # dns servers. name_servers : - 1.1.1.1 - 8.8.8.8 # NTP Servers IP or DNS name, first NTP server will be prefered, and sourced from Managment VRF ntp_servers : - uk.pool.ntp.org - fr.pool.ntp.org","title":"Generic Fabric Information"},{"location":"Arista%20AVD/ansible-avd-config/#configure-fabric-topology","text":"Fabric topology is configured in inventories/inetsix-eapi/group_vars/AVD2_FABRIC.yml which is file that covers AVD2_FABRIC group we defined in inventory . This file contains all the base information to create initial configuration: You can also refer to Arista Validated Design documentation to get a description of every single option available. Subnet to use for underlay, loopback and vtep: # Point to Point Network Summary range, assigned as /31 for each # uplink interfaces # Assign range larger then total [spines * total potential leafs * 2] underlay_p2p_network_summary : 172.31.255.0/24 # IP address range for evpn loopback for all switches in fabric, # assigned as /32s # Assign range larger then total spines + total leafs switches overlay_loopback_network_summary : 192.168.255.0/24 # VTEP VXLAN Tunnel source loopback IP for leaf switches, assigned in /32s # Assign range larger then total leaf switches vtep_loopback_network_summary : 192.168.254.0/24 MLAG IP information # mlag pair IP assignment - assign blocks - Assign range larger then # total spines + total leafs switches mlag_ips : leaf_peer_l3 : 10.255.251.0/24 mlag_peer : 10.255.252.0/24 Then, you have to describe devices for each role. Don\u2019t forget to set management IP here. Spine devices # Spine Switches spine : platform : vEOS-LAB bgp_as : 65001 # defines the range of acceptable remote ASNs from leaf switches leaf_as_range : 65101-65132 nodes : AVD2-SPINE1 : id : 1 mgmt_ip : 10.73.254.1/24 AVD2-SPINE2 : id : 2 mgmt_ip : 10.73.254.2/24 VTEP or L3LEAF devices l3leaf : defaults : virtual_router_mac_address : 00:1c:73:00:dc:01 platform : vEOS-LAB bgp_as : 65100 spines : [ AVD2-SPINE1 , AVD2-SPINE2 ] uplink_to_spine_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] [ ... output truncated ... ] node_groups : AVD2_LEAF1 : bgp_as : 65101 nodes : AVD2-LEAF1A : id : 1 mgmt_ip : 10.73.254.11/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet1 , Ethernet1 ] AVD2-LEAF1B : id : 2 mgmt_ip : 10.73.254.12/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet2 , Ethernet2 ] [ ... output truncated ... ] Aggregation switches: Configuration process is similar to L3LEAFS. Complete documentation of all available variables is available in Arista Validated Design documentation . You can also look at variables part of the demo repo we are using for this post. As you can see here, if you want to change name in your inventory, you have to edit filename in group_vars to reflect that change as well as content of this fabric file.","title":"Configure Fabric topology"},{"location":"Arista%20AVD/ansible-avd-config/#configure-device-type","text":"In each variable file related to a type of devices, we have to instruct AVD what is the role of our devices. --- type : spine # Must be either spine|l3leaf|l2leaf","title":"Configure device type"},{"location":"Arista%20AVD/ansible-avd-config/#configure-vnivlan-across-the-fabric","text":"AVD supports mechanism to create VLANs and VNIs and enable traffic forwarding in your overlay. In current version ( v1.0.2 ), only following design listed below are supported: L2 VLANs Symetric IRB model Model defines a set of tenants (user\u2019s defined) where you can configure VRF or l2vlans or a mix of them. Let\u2019s take a look at how we configure such services. All these configurations shall be configured in file AVD2_TENANTS_NETWORKS.yml","title":"Configure VNI/VLAN across the Fabric."},{"location":"Arista%20AVD/ansible-avd-config/#l2-services","text":"Configure a pure L2 service using EVPN route type 2 only: --- tenants : # Tenant B Specific Information - Pure L2 tenant Tenant_B : mac_vrf_vni_base : 20000 l2vlans : 201 : name : 'B-ELAN-201' tags : [ DC1 ] Tag option allows to configure VLAN only on a subset of the fabric: all devices with this tag will be configured with this vlan. To configure device TAGS and TENANTS options, go to Arista Validated Design documentation In this configuration, VLAN will be created with a tag of 201 and its attached VNI will be configured with 20201 AVD2-LEAF1A#show vlan 201 VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 201 B-ELAN-201 active Po3, Po5, Vx1 AVD2-LEAF1A#show bgp evpn vni 20201 BGP routing table information for VRF default Router identifier 192.168.255.3, local AS number 65101 Route status codes: s - suppressed, * - valid, > - active, # - not installed, E - ECMP head, e - ECMP S - Stale, c - Contributing to ECMP, b - backup % - Pending BGP convergence Origin codes: i - IGP, e - EGP, ? - incomplete AS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop Network Next Hop Metric LocPref Weight Path * > RD: 192.168.255.3:20201 mac-ip 20201 5001.0011.0000 - - - 0 i * > RD: 192.168.255.3:20201 imet 20201 192.168.254.3 - - - 0 i * >Ec RD: 192.168.255.5:20201 imet 20201 192.168.254.5 192.168.254.5 - 100 0 65001 65102 i [... output truncated ...]","title":"L2 Services"},{"location":"Arista%20AVD/ansible-avd-config/#symetric-irb-model","text":"Configure IRB symetric model, use following structure: tenants : # Tenant A Specific Information - VRFs / VLANs Tenant_A : mac_vrf_vni_base : 10000 vrfs : TENANT_A_PROJECT01 : vrf_vni : 11 svis : 110 : name : 'PR01-DMZ' tags : [ DC1 ] enabled : true ip_address_virtual : 10.1.10.254/24 111 : name : 'PR01-TRUST' tags : [ POD02 ] enabled : true ip_address_virtual : 10.1.11.254/24 TENANT_A_PROJECT02 : vrf_vni : 12 vtep_diagnostic : loopback : 100 loopback_ip_range : 10.1.255.0/24 svis : 112 : name : 'PR02-DMZ-GREEN' tags : [ POD01 ] enabled : true ip_address_virtual : 10.1.12.254/24 Example will create 2 VRFs : TENANT_A_PROJECT01 TENANT_A_PROJECT02 In TENANT_A_PROJECT01 , 2 subnets are created and deployed on devices matching TAGS DC1 or POD02 : 10.1.10.0/24 with vlan 110 and vni 10110 10.1.11.0/24 with vlan 111 and vni 10111 In case you deployed this VRF on a MLAG VTEP, an additional vlan is created to allow L3 synchronization within VRF. This vlan is automatically generated with this algorithm: {{ mlag_ibgp_peering_vrfs.base_vlan + (tenants[tenant].vrfs[vrf].vrf_vni - 1) }} In addition to that, each EOS devices will allocate a dynamic VLAN per VRF to support L3 VNI AVD2-LEAF1A#show vlan VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 1 default active Et6, Et7, Et8, PEt6, PEt7, PEt8 110 PR01-DMZ active Cpu, Po3, Po5, Vx1 112 PR02-DMZ-ORANGE active Cpu, Po3, Vx1 201 B-ELAN-201 active Po3, Po5, Vx1 1008* VLAN1008 active Cpu, Po3, Vx1 1009* VLAN1009 active Cpu, Po3, Vx1 3010 MLAG_iBGP_TENANT_A_PROJECT01 active Cpu, Po3 3011 MLAG_iBGP_TENANT_A_PROJECT02 active Cpu, Po3 4093 LEAF_PEER_L3 active Cpu, Po3 4094 MLAG_PEER active Cpu, Po3 * indicates a Dynamic VLAN AVD2-LEAF1A#show vxlan vni VNI to VLAN Mapping for Vxlan1 VNI VLAN Source Interface 802.1Q Tag ----------- ----------- ------------ ------------------- ---------- 11 1008* evpn Vxlan1 1008 12 1009* evpn Vxlan1 1009 10110 110 static Port-Channel5 110 Vxlan1 110 10112 112 static Vxlan1 112 20201 201 static Port-Channel5 201 Vxlan1 201 In TENANT_A_PROJECT02 , we can also see an optional feature named vtep_diagnostic . This option allows you to create a loopback in this VRF and do some connectivity test.","title":"Symetric IRB model"},{"location":"Arista%20AVD/ansible-avd-config/#configure-downlinks","text":"As we have configured L3LS fabric, EVPN/VXLAN overlay, services, it is now time to configure ports to connect servers. Ports should be configured in AVD2_SERVERS.yml . You first have to configure port profile. it is basically a description of how the port will be configured ( access or trunk ) and which set of vlan(s) will be configured --- port_profiles : TENANT_A_B : mode : trunk vlans : \"110-111,201\" A-PR01-DMZ : mode : access vlans : \"110\" This section uses vlan-id so all of these entries must be configured in TENANTS file Then, create port mapping on a per server.","title":"Configure downlinks"},{"location":"Arista%20AVD/ansible-avd-config/#single-home-server","text":"If server is connected to only one leaf to the fabric, following template can be used servers : A-PR01-DMZ-POD01 : # Server name rack : POD01 # Informational RACK adapters : - type : nic server_ports : [ Eth0 ] # Server port to connect switch_ports : [ Ethernet3 ] # Switch port to connect server switches : [ DC1-AGG01 ] # Switch to connect server profile : A-PR01-DMZ # Port profile to apply Whereas most of the information are purely optional as not used by AVD, the last 3 entries are required: switch_ports : Will be used to configure correct port on the switch. switches : Must be switch name defined in your inventory. profile : Profile created previously.","title":"Single home server"},{"location":"Arista%20AVD/ansible-avd-config/#server-connected-to-mlag","text":"In case of connection to MLAG, data structure is the same and only difference is we need to add information about Port-Channel to configure. servers : DCI_RTR01 : rack : DCI adapters : - server_ports : [ Eth1 , Eth2 ] switch_ports : [ Ethernet5 , Ethernet5 ] switches : [ SITE01-BL01A , SITE01-BL01B ] profile : A-PR01-DMZ port_channel : state : present description : PortChannel5 mode : active","title":"Server connected to MLAG"},{"location":"Arista%20AVD/ansible-avd-config/#configure-tasks-for-avd","text":"We will now configure AVD tasks you may use in your playbook:","title":"Configure tasks for AVD"},{"location":"Arista%20AVD/ansible-avd-config/#create-directory-structure","text":"AVD comes with a role to generate your folder structure . tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders vars : fabric_dir_name : '{{fabric_name}}'","title":"Create directory structure"},{"location":"Arista%20AVD/ansible-avd-config/#transform-evpn-datamodel-to-device-data-model","text":"AVD provides role eos_l3ls_evpn role to generate intend YAML device configuration: tasks : - name : generate intend variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn","title":"Transform EVPN datamodel to device data model"},{"location":"Arista%20AVD/ansible-avd-config/#generate-device-configuration-and-documentation","text":"After device data have been generated, AVD can build EOS configuration as well as documentation in markdown format. tasks : - name : generate device intended config and documention tags : [ build ] import_role : name : eos_cli_config_gen","title":"Generate device configuration and documentation"},{"location":"Arista%20AVD/ansible-avd-eve/","text":"Arista AVD on EVE-NG \u00b6 Overview \u00b6 Arista has published a collection named Arista Validated Design to automate build and deploy an EVPN/VXLAN fabric using Red Hat Ansible. This collection supports different deployment scenario such as pure EOS using eAPI or using cloudvision as a change manager. In this post, we will see how to create a local environment to leverage AVD Collection to build EVPN/VXLAN configuration for a set of devices and how to deploy configuration using EOS eAPI. Lab topology \u00b6 Lab topology is running on a EVE-NG server with following elements: vEOS version: Any version from 4.21.8M (lab currently runs 4.24.0F ) Out of band management: 10.255.1.0/24 A jumphost to isolate Out of band management from current network. Jumphost server is running Ubuntu but any Linux flavor should work To make it easy to start, a baseline repository is available on github . You can clone or fork to start working with it. Topology is based on the following design: 1 single site: No DCI feature involved here, only EVPN/VXLAN fabric spread across all nodes. 2 Spines 2 PODs running MLAG VTEP: provide a redundant connectivity from POD to the fabric. 2 POD using single homed VTEP 2 Aggregation switches each attached to its respective MLAG VTEP. 1 POD to emulate DCI connectivity. This POD does not run any specific DCI design and it uses to get visibility of control plane on MLAG out of any server connection. EOS Initial Configuration \u00b6 Management interface of all Arista EOS devices are connected to a Private Cloud in our EVE-NG topology and they are all using the follwing address plan: Spine 01: 10.73.254.1 Spine 02: 10.73.254.2 Leaf 01A: 10.73.254.11 Leaf 01B: 10.73.254.12 Leaf 02A: 10.73.254.13 Leaf02B: 10.73.254.14 Border 01A: 10.73.254.15 Border 01B: 10.73.254.16 Leaf 03A: 10.73.254.17 Leaf 04A: 10.73.254.18 AGG01: 10.73.254.21 AGG02: 10.73.254.22 Initial configuration deployed to devices is based on the following short template: ! transceiver qsfp default-mode 4x10G ! hostname {{ hostname }} ip name-server vrf MGMT 10.255.1.253 ! ntp local-interface vrf MGMT Management1 ntp server vrf MGMT 10.255.1.1 prefer ! spanning-tree mode mstp spanning-tree mst 0 priority 16384 ! no aaa root ! username admin privilege 15 role network-admin secret sha512 {{ password_sha512 }} ! vrf instance MGMT ! interface Ethernet1 ! interface Ethernet2 ! interface Ethernet3 ! interface Ethernet4 ! interface Ethernet5 ! interface Ethernet6 ! interface Ethernet7 ! interface Ethernet8 ! interface Management1 description oob_management vrf MGMT ip address {{ management_ip }} /24 ! ip routing no ip routing vrf MGMT ! management api http-commands no shutdown ! vrf MGMT no shutdown ! end Jumphost Configuration \u00b6 Because our OOB network is completely isolated from our network, a jumphost will be configured to expose eAPI port for all EOS devices. ens3 is configured in OOB network with ip address 10.73.254.253/24 and an IPtables configuration is deployed to serve NAT. This script must be executed with root permissions. Script maps port 443 of EOS to a port on 800x range. $ cat expose-eapi.sh #!/bin/bash echo '* Activate IP Forwarding' sysctl -w net.ipv4.ip_forward = 1 echo '* Flush Current IPTables settings' iptables --flush iptables --delete-chain iptables --table nat --flush iptables --table nat --delete-chain echo '* Activate masquerading' iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE iptables -t nat -A POSTROUTING -o ens2 -j MASQUERADE echo '* Activate eAPI forwarding with base port 800x' iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8001 -j DNAT --to-destination 10 .73.254.1:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8002 -j DNAT --to-destination 10 .73.254.2:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8011 -j DNAT --to-destination 10 .73.254.11:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8012 -j DNAT --to-destination 10 .73.254.12:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8013 -j DNAT --to-destination 10 .73.254.13:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8014 -j DNAT --to-destination 10 .73.254.14:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8015 -j DNAT --to-destination 10 .73.254.15:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8016 -j DNAT --to-destination 10 .73.254.16:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8017 -j DNAT --to-destination 10 .73.254.17:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8018 -j DNAT --to-destination 10 .73.254.18:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8021 -j DNAT --to-destination 10 .73.254.21:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8022 -j DNAT --to-destination 10 .73.254.22:443 echo '* Activate Forward' iptables -A FORWARD -p tcp -d 10 .73.254.0/24 --dport 443 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT echo '* Process done' You can validate everything is well configured with commands below: Check NAT Settings \u00b6 It is important to validate NAT settings are all set as expected $ sudo iptables -t nat -L -n [ sudo ] password for jumper: Chain PREROUTING ( policy ACCEPT ) target prot opt source destination DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8001 to:10.73.254.1:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8002 to:10.73.254.2:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8011 to:10.73.254.11:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8012 to:10.73.254.12:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8013 to:10.73.254.13:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8014 to:10.73.254.14:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8015 to:10.73.254.15:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8016 to:10.73.254.16:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8017 to:10.73.254.17:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8018 to:10.73.254.18:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8021 to:10.73.254.21:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8022 to:10.73.254.22:443 Chain INPUT ( policy ACCEPT ) target prot opt source destination Chain OUTPUT ( policy ACCEPT ) target prot opt source destination Chain POSTROUTING ( policy ACCEPT ) target prot opt source destination MASQUERADE all -- 0 .0.0.0/0 0 .0.0.0/0 MASQUERADE all -- 0 .0.0.0/0 0 .0.0.0/0 Check access to eAPI \u00b6 Once NAT as been correctly set, check you can reach your devices using jumphost address from your laptop $ curl -i -k https://10.73.1.17:8001 HTTP/1.1 301 Moved Permanently Server: nginx Date: Wed, 27 May 2020 08 :09:52 GMT Content-Type: text/html Content-Length: 13 Connection: keep-alive Cache-control: no-store Cache-control: no-cache Cache-control: must-revalidate Cache-control: max-age = 0 Cache-control: pre-check = 0 Cache-control: post-check = 0 Pragma: no-cache Location: https://10.73.1.17:8001/explorer.html X-Frame-Options: DENY Content-Security-Policy: frame-ancestors 'none' Page redirect Create Arista Validated Design project. \u00b6 Ok, enough of lab building and we can start to play with Ansible and AVD collection. Create a repository \u00b6 You can organize your work in many different way, but a structure I find useful is something like this: A folder for all your inventories with one sub-folder per inventory. An inventory folder contains all your variables for a given environment like host_vars , group_vars , inventory.yml A folder to store all playbooks. So it is easy to reuse playbooks whatever the inventory is (if you use a coherent syntax) ansible.cfg at the root of this repository $ tree -L 3 -d . \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 inetsix-cvp \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2502 \u2514\u2500\u2500 media \u2502 \u2514\u2500\u2500 inetsix-eapi \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 medias \u2514\u2500\u2500 playbooks From there, you can run ansible playbooks with following syntax (as an example) : $ ansible-playbook playbooks/my-playbook -i inventories/inetsix-eapi So to make it easy to start, you can clone this repository as baseline: $ git clone https://github.com/inetsix/demo-avd-evpn-eve-ng.git Configure Arista Valdiated Design \u00b6 Arista validated design can be installed in 2 different ways: Ansible Galaxy to get a stable version. Git to use latest development version. In this post we will only use ansible-galaxy command to install this collection. If you want to use very latest version, please refer to Github repository to see how to install using git. # install python requirements $ wget https://github.com/aristanetworks/ansible-avd/blob/devel/development/requirements.txt $ pip3 install -r requirements.txt # install collection $ ansible-galaxy collection install arista.avd Configure Arista Validate Design variables \u00b6 To generate content based on AVD collection, we have to configure 2 things: Inventory file ( inventory.yml ) to list devices in their respective groups group_vars/ files to provide all information required. Information in inventory.yml and within group_vars folder are linked all together. So when changing a hostname may require to edit couple of files. Configure connection information \u00b6 In file inventories/inetsix-eapi/group_vars/all.yml , use the following information: --- ansible_connection : httpapi ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false ansible_network_os : eos ansible_user : admin ansible_ssh_pass : arista123 ansible_become : yes ansible_become_method : enable ansible_host : <YOUR JUMPHOST IP> Configure Arista Valdiated Design variables \u00b6 You can check this post where we detail how to use and configure AVD to build EVPN/VXLAN configuration. And you can also read complete documentation from the project repository . Create a playbook to deploy \u00b6 To deploy configuration, we need to create a playbook under playbooks/ folder. This playbook will do 4 different tasks: Build output directories. Transform EVPN data into YAML files per devices. Build configuration and documentation. Deploy configuration and do backup --- - name : Build Switch configuration hosts : all collections : - arista.avd tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders vars : fabric_dir_name : '{{fabric_name}}' - name : generate intend variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn - name : generate device intended config and documentation tags : [ build ] import_role : name : arista.avd.eos_cli_config_gen - name : deploy configuration to device tags : [ deploy , never ] import_role : name : arista.avd.eos_config_deploy_eapi As you can see, we put 2 different ansible tags to be more granular: build : give us an option to just generate configuration and review offline all the config files. deploy : is set to explicit and push your new configuration to device. $ ansible-playbook playbooks/avd-eapi-generic.yml -i inventories/inetsix-eapi/inventory.yml --tags build Playbook will create all files and folder in your inventory and here is an output example: tree -L 3 -d . \u251c\u2500\u2500 configlets \u251c\u2500\u2500 cvp-debug-logs \u251c\u2500\u2500 inventories \u2502 \u2514\u2500\u2500 inetsix-eapi \u2502 \u251c\u2500\u2500 config_backup \u2502 \u251c\u2500\u2500 documentation \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 intended \u251c\u2500\u2500 output.variables \u2514\u2500\u2500 playbooks To push configuration to devices, just use the other tags: $ ansible-playbook playbooks/avd-eapi-generic.yml -i inventories/inetsix-eapi/inventory.yml --tags \"build, deploy\" Check & Verification \u00b6 If it is first time devices activate arBgp, it is required to reboot topology Connect to your devices and validate your configuration has been deployed: AVD2-LEAF1A login: admin Password: AVD2-LEAF1A>en AVD2-LEAF1A#show run ! Command: show running-config ! device: AVD2-LEAF1A ( vEOS, EOS-4.24.0F ) ! ! boot system flash:/vEOS-lab.swi ! vlan internal order ascending range 1006 1199 ! transceiver qsfp default-mode 4x10G ! [ ... output truncated ... ] You can also check LLDP neighbor: AVD2-LEAF1A#show lldp neighbors Last table change time : 2 days, 20:37:13 ago Number of table inserts : 25 Number of table deletes : 9 Number of table drops : 0 Number of table age-outs : 0 Port Neighbor Device ID Neighbor Port ID TTL Et1 AVD2-SPINE1 Ethernet1 120 Et2 AVD2-SPINE2 Ethernet1 120 Et3 AVD2-LEAF1B Ethernet3 120 Et4 AVD2-LEAF1B Ethernet4 120 Et5 AVD2-AGG01 Ethernet1 120 Ma1 AVD2-BL01A Management1 120 Ma1 AVD2-LEAF2A Management1 120 Ma1 AVD2-LEAF3A Management1 120 Ma1 AVD2-AGG01 Management1 120 Ma1 AVD2-BL01B Management1 120 Ma1 AVD2-LEAF4A Management1 120 Ma1 AVD2-AGG02 Management1 120 Ma1 AVD2-LEAF2B Management1 120 Ma1 AVD2-LEAF1B Management1 120 Ma1 AVD2-SPINE2 Management1 120 Ma1 AVD2-SPINE1 Management1 120 Meantime, MLAG should be all set: AVD2-LEAF1A#show mlag MLAG Configuration: domain-id : AVD2_LEAF1 local-interface : Vlan4094 peer-address : 10 .255.252.1 peer-link : Port-Channel3 hb-peer-address : 10 .73.254.12 hb-peer-vrf : MGMT peer-config : consistent MLAG Status: state : Active negotiation status : Connected peer-link status : Up local-int status : Up system-id : 0e:1d:c0:f0:bd:d1 dual-primary detection : Configured MLAG Ports: Disabled : 0 Configured : 0 Inactive : 0 Active-partial : 0 Active-full : 1 AVD2-LEAF1A#show mlag interfaces local/remote mlag desc state local remote status -------- -------------------- --------------- --------- ---------- ------------ 5 AVD2_L2LEAF1_Po1 active-full Po5 Po5 up/up AVD2-LEAF1A# Based on our setup, you can also check that L3LEAF to AGG devices are well configured: AVD2-LEAF1A#show inter description| grep AGG Et5 up up AVD2-AGG01_Ethernet1 AVD2-LEAF1A#show lacp neighbor State: A = Active, P = Passive; S=ShortTimeout, L=LongTimeout; G = Aggregable, I = Individual; s+=InSync, s-=OutOfSync; C = Collecting, X = state machine expired, D = Distributing, d = default neighbor state | Partner Port Status | Sys-id Port# State OperKey PortPri ------ ----------|------------------------- ------- --------- --------- ------- Port Channel Port-Channel3: Et3 Bundled | 8000,0c-1d-c0-f4-7b-39 3 ALGs+CD 0x0003 32768 Et4 Bundled | 8000,0c-1d-c0-f4-7b-39 4 ALGs+CD 0x0003 32768 Port Channel Port-Channel5*: Et5 Bundled | 8000,0c-1d-c0-90-13-79 1 ALGs+CD 0x0001 32768 * - Only local interfaces for MLAGs are displayed. Connect to the peer to see the state for peer interfaces. And then check your BGP sessions: IPv4 Underlay sessions AVD2-LEAF1A#show bgp ipv4 unicast summary BGP summary information for VRF default Router identifier 192.168.255.3, local AS number 65101 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 10.255.251.1 4 65101 4910 4894 0 0 2d20h Estab 14 14 172.31.255.0 4 65001 4907 4899 0 0 2d21h Estab 11 11 172.31.255.2 4 65001 4897 4892 0 0 2d21h Estab 11 11 EVPN Overlay sessions AVD2-LEAF1A#show bgp evpn summary BGP summary information for VRF default Router identifier 192.168.255.3, local AS number 65101 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 192.168.255.1 4 65001 4931 4914 0 0 2d21h Estab 22 22 192.168.255.2 4 65001 4938 4939 0 0 2d21h Estab 22 22 Then you are all set to continue your journey with Arista Validated design References \u00b6 Arista Recommended design Guide Arista Validated Deisgn EVE-NG website How to configure AVD","title":"Arista AVD on EVE-NG"},{"location":"Arista%20AVD/ansible-avd-eve/#arista-avd-on-eve-ng","text":"","title":"Arista AVD on EVE-NG"},{"location":"Arista%20AVD/ansible-avd-eve/#overview","text":"Arista has published a collection named Arista Validated Design to automate build and deploy an EVPN/VXLAN fabric using Red Hat Ansible. This collection supports different deployment scenario such as pure EOS using eAPI or using cloudvision as a change manager. In this post, we will see how to create a local environment to leverage AVD Collection to build EVPN/VXLAN configuration for a set of devices and how to deploy configuration using EOS eAPI.","title":"Overview"},{"location":"Arista%20AVD/ansible-avd-eve/#lab-topology","text":"Lab topology is running on a EVE-NG server with following elements: vEOS version: Any version from 4.21.8M (lab currently runs 4.24.0F ) Out of band management: 10.255.1.0/24 A jumphost to isolate Out of band management from current network. Jumphost server is running Ubuntu but any Linux flavor should work To make it easy to start, a baseline repository is available on github . You can clone or fork to start working with it. Topology is based on the following design: 1 single site: No DCI feature involved here, only EVPN/VXLAN fabric spread across all nodes. 2 Spines 2 PODs running MLAG VTEP: provide a redundant connectivity from POD to the fabric. 2 POD using single homed VTEP 2 Aggregation switches each attached to its respective MLAG VTEP. 1 POD to emulate DCI connectivity. This POD does not run any specific DCI design and it uses to get visibility of control plane on MLAG out of any server connection.","title":"Lab topology"},{"location":"Arista%20AVD/ansible-avd-eve/#eos-initial-configuration","text":"Management interface of all Arista EOS devices are connected to a Private Cloud in our EVE-NG topology and they are all using the follwing address plan: Spine 01: 10.73.254.1 Spine 02: 10.73.254.2 Leaf 01A: 10.73.254.11 Leaf 01B: 10.73.254.12 Leaf 02A: 10.73.254.13 Leaf02B: 10.73.254.14 Border 01A: 10.73.254.15 Border 01B: 10.73.254.16 Leaf 03A: 10.73.254.17 Leaf 04A: 10.73.254.18 AGG01: 10.73.254.21 AGG02: 10.73.254.22 Initial configuration deployed to devices is based on the following short template: ! transceiver qsfp default-mode 4x10G ! hostname {{ hostname }} ip name-server vrf MGMT 10.255.1.253 ! ntp local-interface vrf MGMT Management1 ntp server vrf MGMT 10.255.1.1 prefer ! spanning-tree mode mstp spanning-tree mst 0 priority 16384 ! no aaa root ! username admin privilege 15 role network-admin secret sha512 {{ password_sha512 }} ! vrf instance MGMT ! interface Ethernet1 ! interface Ethernet2 ! interface Ethernet3 ! interface Ethernet4 ! interface Ethernet5 ! interface Ethernet6 ! interface Ethernet7 ! interface Ethernet8 ! interface Management1 description oob_management vrf MGMT ip address {{ management_ip }} /24 ! ip routing no ip routing vrf MGMT ! management api http-commands no shutdown ! vrf MGMT no shutdown ! end","title":"EOS Initial Configuration"},{"location":"Arista%20AVD/ansible-avd-eve/#jumphost-configuration","text":"Because our OOB network is completely isolated from our network, a jumphost will be configured to expose eAPI port for all EOS devices. ens3 is configured in OOB network with ip address 10.73.254.253/24 and an IPtables configuration is deployed to serve NAT. This script must be executed with root permissions. Script maps port 443 of EOS to a port on 800x range. $ cat expose-eapi.sh #!/bin/bash echo '* Activate IP Forwarding' sysctl -w net.ipv4.ip_forward = 1 echo '* Flush Current IPTables settings' iptables --flush iptables --delete-chain iptables --table nat --flush iptables --table nat --delete-chain echo '* Activate masquerading' iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE iptables -t nat -A POSTROUTING -o ens2 -j MASQUERADE echo '* Activate eAPI forwarding with base port 800x' iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8001 -j DNAT --to-destination 10 .73.254.1:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8002 -j DNAT --to-destination 10 .73.254.2:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8011 -j DNAT --to-destination 10 .73.254.11:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8012 -j DNAT --to-destination 10 .73.254.12:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8013 -j DNAT --to-destination 10 .73.254.13:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8014 -j DNAT --to-destination 10 .73.254.14:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8015 -j DNAT --to-destination 10 .73.254.15:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8016 -j DNAT --to-destination 10 .73.254.16:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8017 -j DNAT --to-destination 10 .73.254.17:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8018 -j DNAT --to-destination 10 .73.254.18:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8021 -j DNAT --to-destination 10 .73.254.21:443 iptables -t nat -A PREROUTING -p tcp -i ens1 --dport 8022 -j DNAT --to-destination 10 .73.254.22:443 echo '* Activate Forward' iptables -A FORWARD -p tcp -d 10 .73.254.0/24 --dport 443 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT echo '* Process done' You can validate everything is well configured with commands below:","title":"Jumphost Configuration"},{"location":"Arista%20AVD/ansible-avd-eve/#check-nat-settings","text":"It is important to validate NAT settings are all set as expected $ sudo iptables -t nat -L -n [ sudo ] password for jumper: Chain PREROUTING ( policy ACCEPT ) target prot opt source destination DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8001 to:10.73.254.1:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8002 to:10.73.254.2:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8011 to:10.73.254.11:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8012 to:10.73.254.12:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8013 to:10.73.254.13:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8014 to:10.73.254.14:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8015 to:10.73.254.15:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8016 to:10.73.254.16:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8017 to:10.73.254.17:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8018 to:10.73.254.18:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8021 to:10.73.254.21:443 DNAT tcp -- 0 .0.0.0/0 0 .0.0.0/0 tcp dpt:8022 to:10.73.254.22:443 Chain INPUT ( policy ACCEPT ) target prot opt source destination Chain OUTPUT ( policy ACCEPT ) target prot opt source destination Chain POSTROUTING ( policy ACCEPT ) target prot opt source destination MASQUERADE all -- 0 .0.0.0/0 0 .0.0.0/0 MASQUERADE all -- 0 .0.0.0/0 0 .0.0.0/0","title":"Check NAT Settings"},{"location":"Arista%20AVD/ansible-avd-eve/#check-access-to-eapi","text":"Once NAT as been correctly set, check you can reach your devices using jumphost address from your laptop $ curl -i -k https://10.73.1.17:8001 HTTP/1.1 301 Moved Permanently Server: nginx Date: Wed, 27 May 2020 08 :09:52 GMT Content-Type: text/html Content-Length: 13 Connection: keep-alive Cache-control: no-store Cache-control: no-cache Cache-control: must-revalidate Cache-control: max-age = 0 Cache-control: pre-check = 0 Cache-control: post-check = 0 Pragma: no-cache Location: https://10.73.1.17:8001/explorer.html X-Frame-Options: DENY Content-Security-Policy: frame-ancestors 'none' Page redirect","title":"Check access to eAPI"},{"location":"Arista%20AVD/ansible-avd-eve/#create-arista-validated-design-project","text":"Ok, enough of lab building and we can start to play with Ansible and AVD collection.","title":"Create Arista Validated Design project."},{"location":"Arista%20AVD/ansible-avd-eve/#create-a-repository","text":"You can organize your work in many different way, but a structure I find useful is something like this: A folder for all your inventories with one sub-folder per inventory. An inventory folder contains all your variables for a given environment like host_vars , group_vars , inventory.yml A folder to store all playbooks. So it is easy to reuse playbooks whatever the inventory is (if you use a coherent syntax) ansible.cfg at the root of this repository $ tree -L 3 -d . \u251c\u2500\u2500 inventories \u2502 \u251c\u2500\u2500 inetsix-cvp \u2502 \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2502 \u2514\u2500\u2500 media \u2502 \u2514\u2500\u2500 inetsix-eapi \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 medias \u2514\u2500\u2500 playbooks From there, you can run ansible playbooks with following syntax (as an example) : $ ansible-playbook playbooks/my-playbook -i inventories/inetsix-eapi So to make it easy to start, you can clone this repository as baseline: $ git clone https://github.com/inetsix/demo-avd-evpn-eve-ng.git","title":"Create a repository"},{"location":"Arista%20AVD/ansible-avd-eve/#configure-arista-valdiated-design","text":"Arista validated design can be installed in 2 different ways: Ansible Galaxy to get a stable version. Git to use latest development version. In this post we will only use ansible-galaxy command to install this collection. If you want to use very latest version, please refer to Github repository to see how to install using git. # install python requirements $ wget https://github.com/aristanetworks/ansible-avd/blob/devel/development/requirements.txt $ pip3 install -r requirements.txt # install collection $ ansible-galaxy collection install arista.avd","title":"Configure Arista Valdiated Design"},{"location":"Arista%20AVD/ansible-avd-eve/#configure-arista-validate-design-variables","text":"To generate content based on AVD collection, we have to configure 2 things: Inventory file ( inventory.yml ) to list devices in their respective groups group_vars/ files to provide all information required. Information in inventory.yml and within group_vars folder are linked all together. So when changing a hostname may require to edit couple of files.","title":"Configure Arista Validate Design variables"},{"location":"Arista%20AVD/ansible-avd-eve/#configure-connection-information","text":"In file inventories/inetsix-eapi/group_vars/all.yml , use the following information: --- ansible_connection : httpapi ansible_httpapi_port : '{{ansible_port}}' ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false ansible_network_os : eos ansible_user : admin ansible_ssh_pass : arista123 ansible_become : yes ansible_become_method : enable ansible_host : <YOUR JUMPHOST IP>","title":"Configure connection information"},{"location":"Arista%20AVD/ansible-avd-eve/#configure-arista-valdiated-design-variables","text":"You can check this post where we detail how to use and configure AVD to build EVPN/VXLAN configuration. And you can also read complete documentation from the project repository .","title":"Configure Arista Valdiated Design variables"},{"location":"Arista%20AVD/ansible-avd-eve/#create-a-playbook-to-deploy","text":"To deploy configuration, we need to create a playbook under playbooks/ folder. This playbook will do 4 different tasks: Build output directories. Transform EVPN data into YAML files per devices. Build configuration and documentation. Deploy configuration and do backup --- - name : Build Switch configuration hosts : all collections : - arista.avd tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders vars : fabric_dir_name : '{{fabric_name}}' - name : generate intend variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn - name : generate device intended config and documentation tags : [ build ] import_role : name : arista.avd.eos_cli_config_gen - name : deploy configuration to device tags : [ deploy , never ] import_role : name : arista.avd.eos_config_deploy_eapi As you can see, we put 2 different ansible tags to be more granular: build : give us an option to just generate configuration and review offline all the config files. deploy : is set to explicit and push your new configuration to device. $ ansible-playbook playbooks/avd-eapi-generic.yml -i inventories/inetsix-eapi/inventory.yml --tags build Playbook will create all files and folder in your inventory and here is an output example: tree -L 3 -d . \u251c\u2500\u2500 configlets \u251c\u2500\u2500 cvp-debug-logs \u251c\u2500\u2500 inventories \u2502 \u2514\u2500\u2500 inetsix-eapi \u2502 \u251c\u2500\u2500 config_backup \u2502 \u251c\u2500\u2500 documentation \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 intended \u251c\u2500\u2500 output.variables \u2514\u2500\u2500 playbooks To push configuration to devices, just use the other tags: $ ansible-playbook playbooks/avd-eapi-generic.yml -i inventories/inetsix-eapi/inventory.yml --tags \"build, deploy\"","title":"Create a playbook to deploy"},{"location":"Arista%20AVD/ansible-avd-eve/#check-verification","text":"If it is first time devices activate arBgp, it is required to reboot topology Connect to your devices and validate your configuration has been deployed: AVD2-LEAF1A login: admin Password: AVD2-LEAF1A>en AVD2-LEAF1A#show run ! Command: show running-config ! device: AVD2-LEAF1A ( vEOS, EOS-4.24.0F ) ! ! boot system flash:/vEOS-lab.swi ! vlan internal order ascending range 1006 1199 ! transceiver qsfp default-mode 4x10G ! [ ... output truncated ... ] You can also check LLDP neighbor: AVD2-LEAF1A#show lldp neighbors Last table change time : 2 days, 20:37:13 ago Number of table inserts : 25 Number of table deletes : 9 Number of table drops : 0 Number of table age-outs : 0 Port Neighbor Device ID Neighbor Port ID TTL Et1 AVD2-SPINE1 Ethernet1 120 Et2 AVD2-SPINE2 Ethernet1 120 Et3 AVD2-LEAF1B Ethernet3 120 Et4 AVD2-LEAF1B Ethernet4 120 Et5 AVD2-AGG01 Ethernet1 120 Ma1 AVD2-BL01A Management1 120 Ma1 AVD2-LEAF2A Management1 120 Ma1 AVD2-LEAF3A Management1 120 Ma1 AVD2-AGG01 Management1 120 Ma1 AVD2-BL01B Management1 120 Ma1 AVD2-LEAF4A Management1 120 Ma1 AVD2-AGG02 Management1 120 Ma1 AVD2-LEAF2B Management1 120 Ma1 AVD2-LEAF1B Management1 120 Ma1 AVD2-SPINE2 Management1 120 Ma1 AVD2-SPINE1 Management1 120 Meantime, MLAG should be all set: AVD2-LEAF1A#show mlag MLAG Configuration: domain-id : AVD2_LEAF1 local-interface : Vlan4094 peer-address : 10 .255.252.1 peer-link : Port-Channel3 hb-peer-address : 10 .73.254.12 hb-peer-vrf : MGMT peer-config : consistent MLAG Status: state : Active negotiation status : Connected peer-link status : Up local-int status : Up system-id : 0e:1d:c0:f0:bd:d1 dual-primary detection : Configured MLAG Ports: Disabled : 0 Configured : 0 Inactive : 0 Active-partial : 0 Active-full : 1 AVD2-LEAF1A#show mlag interfaces local/remote mlag desc state local remote status -------- -------------------- --------------- --------- ---------- ------------ 5 AVD2_L2LEAF1_Po1 active-full Po5 Po5 up/up AVD2-LEAF1A# Based on our setup, you can also check that L3LEAF to AGG devices are well configured: AVD2-LEAF1A#show inter description| grep AGG Et5 up up AVD2-AGG01_Ethernet1 AVD2-LEAF1A#show lacp neighbor State: A = Active, P = Passive; S=ShortTimeout, L=LongTimeout; G = Aggregable, I = Individual; s+=InSync, s-=OutOfSync; C = Collecting, X = state machine expired, D = Distributing, d = default neighbor state | Partner Port Status | Sys-id Port# State OperKey PortPri ------ ----------|------------------------- ------- --------- --------- ------- Port Channel Port-Channel3: Et3 Bundled | 8000,0c-1d-c0-f4-7b-39 3 ALGs+CD 0x0003 32768 Et4 Bundled | 8000,0c-1d-c0-f4-7b-39 4 ALGs+CD 0x0003 32768 Port Channel Port-Channel5*: Et5 Bundled | 8000,0c-1d-c0-90-13-79 1 ALGs+CD 0x0001 32768 * - Only local interfaces for MLAGs are displayed. Connect to the peer to see the state for peer interfaces. And then check your BGP sessions: IPv4 Underlay sessions AVD2-LEAF1A#show bgp ipv4 unicast summary BGP summary information for VRF default Router identifier 192.168.255.3, local AS number 65101 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 10.255.251.1 4 65101 4910 4894 0 0 2d20h Estab 14 14 172.31.255.0 4 65001 4907 4899 0 0 2d21h Estab 11 11 172.31.255.2 4 65001 4897 4892 0 0 2d21h Estab 11 11 EVPN Overlay sessions AVD2-LEAF1A#show bgp evpn summary BGP summary information for VRF default Router identifier 192.168.255.3, local AS number 65101 Neighbor Status Codes: m - Under maintenance Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 192.168.255.1 4 65001 4931 4914 0 0 2d21h Estab 22 22 192.168.255.2 4 65001 4938 4939 0 0 2d21h Estab 22 22 Then you are all set to continue your journey with Arista Validated design","title":"Check &amp; Verification"},{"location":"Arista%20AVD/ansible-avd-eve/#references","text":"Arista Recommended design Guide Arista Validated Deisgn EVE-NG website How to configure AVD","title":"References"},{"location":"how-to/ci-to-start-docker-and-validate-code/","text":"Continuous Intgration with Docker interactions \u00b6 This article explains how to configure gitlabci runners to run your code and provision target to validate your code. Note: Docker is a requirement on your host device to both run gitlab runner and your project. Configure a gitlab ci runner \u00b6 First step is to configure a gitlab runner and connect it to your projet. Gitlab provides a docker image to deploy their runner. It is available on this page . A default command to start a runner is the following: $ docker run -d --name gitlab-runner \\ --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest This command starts a docker gitlabci runner and share following folder with host filesystem: /var/run/docker.sock is bind to the same path within container In this scenario, configuration is not permanent and will be deleted after we stop the container. And especially configuration related to gitlabci service. To make it permanent, add following line to previous command: $ mkdir -p /home/docker/gitlab-runner $ docker run -d --name gitlab-runner \\ --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /home/docker/gitlab-runner:/etc/gitlab-runner \\ gitlab/gitlab-runner:latest Then, we have to register runner to your gitlab project. Go to https://gitlab.your.server.com/your-namespace/your-project/settings/ci_cd and note both your token and URL Once you have these information, we can register the runner with following command: docker exec -it gitlab-runner gitlab-runner register \\ --non-interactive \\ --url $gitlab .your.server.com$ \\ --registration-token $TOKEN $ \\ --executor \"docker\" \\ --docker-image alpine:3 \\ --description \"docker-runner\" \\ --tag-list $YOUR_TAG \\ --docker-volumes /var/run/docker.sock:/var/run/docker.sock \\ --run-untagged \\ --locked = \"false\"","title":"Continuous Intgration with Docker interactions"},{"location":"how-to/ci-to-start-docker-and-validate-code/#continuous-intgration-with-docker-interactions","text":"This article explains how to configure gitlabci runners to run your code and provision target to validate your code. Note: Docker is a requirement on your host device to both run gitlab runner and your project.","title":"Continuous Intgration with Docker interactions"},{"location":"how-to/ci-to-start-docker-and-validate-code/#configure-a-gitlab-ci-runner","text":"First step is to configure a gitlab runner and connect it to your projet. Gitlab provides a docker image to deploy their runner. It is available on this page . A default command to start a runner is the following: $ docker run -d --name gitlab-runner \\ --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest This command starts a docker gitlabci runner and share following folder with host filesystem: /var/run/docker.sock is bind to the same path within container In this scenario, configuration is not permanent and will be deleted after we stop the container. And especially configuration related to gitlabci service. To make it permanent, add following line to previous command: $ mkdir -p /home/docker/gitlab-runner $ docker run -d --name gitlab-runner \\ --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /home/docker/gitlab-runner:/etc/gitlab-runner \\ gitlab/gitlab-runner:latest Then, we have to register runner to your gitlab project. Go to https://gitlab.your.server.com/your-namespace/your-project/settings/ci_cd and note both your token and URL Once you have these information, we can register the runner with following command: docker exec -it gitlab-runner gitlab-runner register \\ --non-interactive \\ --url $gitlab .your.server.com$ \\ --registration-token $TOKEN $ \\ --executor \"docker\" \\ --docker-image alpine:3 \\ --description \"docker-runner\" \\ --tag-list $YOUR_TAG \\ --docker-volumes /var/run/docker.sock:/var/run/docker.sock \\ --run-untagged \\ --locked = \"false\"","title":"Configure a gitlab ci runner"},{"location":"how-to/docker-ansible/","text":"Ansible using docker \u00b6 This how-to explains how to configure docker and your shell environment to execute ansible within a container and not from your system. It is useful to use this approach to validate code with different ansible version. Github repository : titom73/docker-ansible Docker Image : inetsix/docker-ansible Repository we use in this example gives us option to: share your ssh agent with the ansible docker share your AWS CLI configuration Use a specifc version without installation Use AWS and Junos componants: aws cli , junos-eznc , Juniper.junos ansible module Just add these aliases to your ~/.{bash|zsh|...}_aliases in order to use ansible as it where installed on your computer. export DOCKER_ANSIBLE_VERSION = 2 .5 base_ansible () { docker run -it --rm \\ --volume $SSH_AUTH_SOCK :/ssh-agent \\ --env SSH_AUTH_SOCK = /ssh-agent \\ -v ${ PWD } :project \\ -v ${ HOME } /.ssh/known_hosts:/root/.ssh/known_hosts \\ -w ${ PWD } \\ inetsix/docker-ansible: ${ DOCKER_ANSIBLE_VERSION } $@ } Once this bash function is available, you can build you aliases to mimic ansible: alias ansible-update = 'docker pull inetsix/docker-ansible:${DOCKER_ANSIBLE_VERSION}' alias ansible-shell = 'base_ansible bash' alias ansible = 'base_ansible ansible' alias ansible-playbook = 'base_ansible ansible-playbook' alias ansible-vault = 'base_ansible ansible-vault' alias ansible-galaxy = 'base_ansible ansible-galaxy' The export statement gives option to specify which version of ansible we want to use. This version is equal to tags defined during the build This docker image is built with the following tags (12/2018) for both alpine and Debian jessie OS: latest 2.3 2.4 2.5 2.6 2.7 Usage \u00b6 Simply use your aliases, and you can override ansible default version when required $ DOCKER_ANSIBLE_VERSION = 2 .5 $ docker-ansible --version ansible 2 .5.2 config file = /Users/titom73/Scripting/ansible.projects/ansible.training.phase2/ansible.cfg configured module search path = [ u '/root/.ansible/plugins/modules' , \\ u '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python2.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 2 .7.14 ( default, Apr 27 2018 , 09 :37:08 ) [ GCC 4 .9.2 ] Optional parameters \u00b6 Export AWS CLI parameters \u00b6 Assuming you have configured your AWS CLI, you may export your parameters with the following variables: On your local shell: export AWS_DEFAULT_REGION = 'eu-west-2' export AWS_PROFILE = 'your_aws_profile' Then, these variables are exposed to your docker instance by using --env keyword. If they are not set, then, nothing is export to your container. export DOCKER_ANSIBLE_VERSION = 2 .5 base_ansible () { docker run -it --rm \\ --env ANSIBLE_REMOTE_USER = ${ USER } \\ --env AWS_DEFAULT_REGION = ${ AWS_DEFAULT_REGION } \\ --env AWS_PROFILE = ${ AWS_PROFILE } \\ --volume $SSH_AUTH_SOCK :/ssh-agent \\ --env SSH_AUTH_SOCK = /ssh-agent \\ -v ${ PWD } :project \\ -v ${ HOME } /.aws/:/root/.aws/ \\ -v ${ HOME } /.ssh/known_hosts:/root/.ssh/known_hosts \\ -w ${ PWD } \\ inetsix/docker-ansible: ${ DOCKER_ANSIBLE_VERSION } $@ } Overwrite ansible commands \u00b6 If for some reasons, you need to use your installed ansible version, just use \\\\ in front of any ansible command like below: $ \\a nsible --version ansible 2 .5.1 config file = /Users/titom73/Scripting/ansible.projects/ansible.training.phase2/ansible.cfg configured module search path = [ u '/Users/titom73/.ansible/plugins/modules' , \\ u '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python2.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 2 .7.14 ( default, Mar 22 2018 , 15 :04:47 ) \\ [ GCC 4 .2.1 Compatible Apple LLVM 9 .0.0 ( clang-900.0.39.2 )] $ ansible --version ansible 2 .5.2 config file = /Users/titom73/Scripting/ansible.projects/ansible.training.phase2/ansible.cfg configured module search path = [ u '/root/.ansible/plugins/modules' , \\ u '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python2.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 2 .7.14 ( default, Apr 27 2018 , 09 :37:08 ) [ GCC 4 .9.2 ]","title":"Ansible using docker"},{"location":"how-to/docker-ansible/#ansible-using-docker","text":"This how-to explains how to configure docker and your shell environment to execute ansible within a container and not from your system. It is useful to use this approach to validate code with different ansible version. Github repository : titom73/docker-ansible Docker Image : inetsix/docker-ansible Repository we use in this example gives us option to: share your ssh agent with the ansible docker share your AWS CLI configuration Use a specifc version without installation Use AWS and Junos componants: aws cli , junos-eznc , Juniper.junos ansible module Just add these aliases to your ~/.{bash|zsh|...}_aliases in order to use ansible as it where installed on your computer. export DOCKER_ANSIBLE_VERSION = 2 .5 base_ansible () { docker run -it --rm \\ --volume $SSH_AUTH_SOCK :/ssh-agent \\ --env SSH_AUTH_SOCK = /ssh-agent \\ -v ${ PWD } :project \\ -v ${ HOME } /.ssh/known_hosts:/root/.ssh/known_hosts \\ -w ${ PWD } \\ inetsix/docker-ansible: ${ DOCKER_ANSIBLE_VERSION } $@ } Once this bash function is available, you can build you aliases to mimic ansible: alias ansible-update = 'docker pull inetsix/docker-ansible:${DOCKER_ANSIBLE_VERSION}' alias ansible-shell = 'base_ansible bash' alias ansible = 'base_ansible ansible' alias ansible-playbook = 'base_ansible ansible-playbook' alias ansible-vault = 'base_ansible ansible-vault' alias ansible-galaxy = 'base_ansible ansible-galaxy' The export statement gives option to specify which version of ansible we want to use. This version is equal to tags defined during the build This docker image is built with the following tags (12/2018) for both alpine and Debian jessie OS: latest 2.3 2.4 2.5 2.6 2.7","title":"Ansible using docker"},{"location":"how-to/docker-ansible/#usage","text":"Simply use your aliases, and you can override ansible default version when required $ DOCKER_ANSIBLE_VERSION = 2 .5 $ docker-ansible --version ansible 2 .5.2 config file = /Users/titom73/Scripting/ansible.projects/ansible.training.phase2/ansible.cfg configured module search path = [ u '/root/.ansible/plugins/modules' , \\ u '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python2.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 2 .7.14 ( default, Apr 27 2018 , 09 :37:08 ) [ GCC 4 .9.2 ]","title":"Usage"},{"location":"how-to/docker-ansible/#optional-parameters","text":"","title":"Optional parameters"},{"location":"how-to/docker-ansible/#export-aws-cli-parameters","text":"Assuming you have configured your AWS CLI, you may export your parameters with the following variables: On your local shell: export AWS_DEFAULT_REGION = 'eu-west-2' export AWS_PROFILE = 'your_aws_profile' Then, these variables are exposed to your docker instance by using --env keyword. If they are not set, then, nothing is export to your container. export DOCKER_ANSIBLE_VERSION = 2 .5 base_ansible () { docker run -it --rm \\ --env ANSIBLE_REMOTE_USER = ${ USER } \\ --env AWS_DEFAULT_REGION = ${ AWS_DEFAULT_REGION } \\ --env AWS_PROFILE = ${ AWS_PROFILE } \\ --volume $SSH_AUTH_SOCK :/ssh-agent \\ --env SSH_AUTH_SOCK = /ssh-agent \\ -v ${ PWD } :project \\ -v ${ HOME } /.aws/:/root/.aws/ \\ -v ${ HOME } /.ssh/known_hosts:/root/.ssh/known_hosts \\ -w ${ PWD } \\ inetsix/docker-ansible: ${ DOCKER_ANSIBLE_VERSION } $@ }","title":"Export AWS CLI parameters"},{"location":"how-to/docker-ansible/#overwrite-ansible-commands","text":"If for some reasons, you need to use your installed ansible version, just use \\\\ in front of any ansible command like below: $ \\a nsible --version ansible 2 .5.1 config file = /Users/titom73/Scripting/ansible.projects/ansible.training.phase2/ansible.cfg configured module search path = [ u '/Users/titom73/.ansible/plugins/modules' , \\ u '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python2.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 2 .7.14 ( default, Mar 22 2018 , 15 :04:47 ) \\ [ GCC 4 .2.1 Compatible Apple LLVM 9 .0.0 ( clang-900.0.39.2 )] $ ansible --version ansible 2 .5.2 config file = /Users/titom73/Scripting/ansible.projects/ansible.training.phase2/ansible.cfg configured module search path = [ u '/root/.ansible/plugins/modules' , \\ u '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python2.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 2 .7.14 ( default, Apr 27 2018 , 09 :37:08 ) [ GCC 4 .9.2 ]","title":"Overwrite ansible commands"},{"location":"how-to/k8s-tips-tricks/","text":"Kubernetes Tips \u00b6 Complete version available on titom73/k8s-journey Tips & general settings \u00b6 Shell integration \u00b6 Plugin for oh-my-zsh vim ~/.zshrc plugins =( ... kubectl ) Get a shell in a K8S container \u00b6 Get a shell in namespace: $ kubectl run --restart = Never --rm -it --image = alpine alpine-runner Set alias to start shell in k8s alias kalpine = 'kubectl run --restart=Never --rm -it --image=alpine alpine-runner' Namespace Switcher \u00b6 Install kns $ brew tap blendle/blendle $ brew install kns Test HTTP Load Balancing \u00b6 $ export SERVER = 1 .1.1.1 $ export PORT = 32081 $ while sleep 0 .3 ; do curl -s ${ SERVER } : ${ PORT } | grep name ; done Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-mv26c\" Server name: \"httpenv-cfb65dd68-mv26c\" Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-mv26c\" Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-mv26c Kill POD \u00b6 Do not wait grace period for signal propagation. # Delete a pod with minimal delay $ kubectl delete pod foo --now # Force delete a pod on a dead node $ kubectl delete pod foo --force $ kubectl delete pod foo --grace-period = -1 # Period of time in seconds given to the resource to # terminate gracefully. Ignored if negative. # Set to 1 for immediate shutdown. Can only be set # to 0 when --force is true (force deletion). Deployment / Replicaset / POD relations \u00b6 Deployment always create a replicaSet A replicaSet always create at least one POD $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE pingpong 1 /1 1 1 4m25s $ kubectl get rs NAME DESIRED CURRENT READY AGE pingpong-6777998c97 1 1 1 4m29s $ kubectl get pods NAME READY STATUS RESTARTS AGE pingpong-6777998c97-l8dmk 1 /1 Running 0 4m34s Scale deployment \u00b6 Instantiate scale up process \u00b6 $ kubectl scale deployment nginx-basics --replicas = 3 deployment.apps/nginx-basics scaled Check K8S status \u00b6 $ kubectl get deployments.apps nginx-basics NAME READY UP-TO-DATE AVAILABLE AGE nginx-basics 3 /3 3 3 19h $ kubectl get pods --selector = app = webui NAME READY STATUS RESTARTS AGE nginx-basics-c48d789fd-kzcbt 1 /1 Running 1 19h nginx-basics-c48d789fd-vbrmv 1 /1 Running 0 3m39s nginx-basics-c48d789fd-vx9t5 1 /1 Running 0 3m39s $ kubectl describe deployments.apps nginx-basics Name: nginx-basics Namespace: default CreationTimestamp: Tue, 23 Jun 2020 15 :44:44 +0200 Labels: <none> Annotations: deployment.kubernetes.io/revision: 1 kubectl.kubernetes.io/last-applied-configuration: ... Selector: app = webui Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25 % max unavailable, 25 % max surge Pod Template: Labels: app = webui Containers: nginx: Image: titom73/nginx Port: 80 /TCP Host Port: 0 /TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: <none> NewReplicaSet: nginx-basics-c48d789fd ( 3 /3 replicas created ) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 4m5s deployment-controller Scaled up replica set nginx-basics-c48d789fd to 3 Check with Server requests \u00b6 $ while sleep 0 .3 ; do curl -s test.lab.as73.inetsix.net/basics | grep name ; done Server name: nginx-basics-c48d789fd-kzcbt Server name: nginx-basics-c48d789fd-vbrmv Server name: nginx-basics-c48d789fd-vx9t5 Server name: nginx-basics-c48d789fd-kzcbt Server name: nginx-basics-c48d789fd-vbrmv Tricks \u00b6 DNS configuration \u00b6 $ microk8s enable dns $ microk8s kubectl -n kube-system edit configmap/coredns # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready log . { class error } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 8 .8.8.8 8 .8.4.4 cache 30 loop reload loadbalance } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | [ ... output truncated ... ] Issue with DNS resolution on Alpine \u00b6 k8s-journey/manifests on \ue0a0 master [?] at \u2638\ufe0f microk8s \u279c kubectl describe pod pingpong-8558fd45c7-pqc7p Name: pingpong-8558fd45c7-pqc7p Namespace: default Priority: 0 Node: k8s-node01/10.73.1.248 Start Time: Mon, 22 Jun 2020 22:50:15 +0200 Labels: app=pingpong pod-template-hash=8558fd45c7 [... output truncated ...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m38s default-scheduler Successfully assigned default/pingpong-8558fd45c7-pqc7p to k8s-node01 Normal Pulling 4m22s (x3 over 4m37s) kubelet, k8s-node01 Pulling image \"alpine\" Normal Pulled 4m22s (x3 over 4m36s) kubelet, k8s-node01 Successfully pulled image \"alpine\" Normal Created 4m21s (x3 over 4m36s) kubelet, k8s-node01 Created container alpine Normal Started 4m21s (x3 over 4m36s) kubelet, k8s-node01 Started container alpine Warning BackOff 4m9s (x4 over 4m34s) kubelet, k8s-node01 Back-off restarting failed container Warning DNSConfigForming 3m55s (x9 over 4m38s) kubelet, k8s-node01 Search Line limits were exceeded, some search paths have been omitted, the applied search line is: default.svc.cluster.local svc.cluster.local cluster.local lab.as73.inetsix.net as73047inetsix.net inetsix.net Reason is search name in DNS cannot exceed 6 dns domains : const ( // Limits on various DNS parameters. These are derived from // restrictions in Linux libc name resolution handling. // Max number of DNS name servers. MaxDNSNameservers = 3 // Max number of domains in search path. MaxDNSSearchPaths = 6 // Max number of characters in search path. MaxDNSSearchListChars = 256 ) Discussion on Stackoverflow","title":"Kubernetes Tips"},{"location":"how-to/k8s-tips-tricks/#kubernetes-tips","text":"Complete version available on titom73/k8s-journey","title":"Kubernetes Tips"},{"location":"how-to/k8s-tips-tricks/#tips-general-settings","text":"","title":"Tips &amp; general settings"},{"location":"how-to/k8s-tips-tricks/#shell-integration","text":"Plugin for oh-my-zsh vim ~/.zshrc plugins =( ... kubectl )","title":"Shell integration"},{"location":"how-to/k8s-tips-tricks/#get-a-shell-in-a-k8s-container","text":"Get a shell in namespace: $ kubectl run --restart = Never --rm -it --image = alpine alpine-runner Set alias to start shell in k8s alias kalpine = 'kubectl run --restart=Never --rm -it --image=alpine alpine-runner'","title":"Get a shell in a K8S container"},{"location":"how-to/k8s-tips-tricks/#namespace-switcher","text":"Install kns $ brew tap blendle/blendle $ brew install kns","title":"Namespace Switcher"},{"location":"how-to/k8s-tips-tricks/#test-http-load-balancing","text":"$ export SERVER = 1 .1.1.1 $ export PORT = 32081 $ while sleep 0 .3 ; do curl -s ${ SERVER } : ${ PORT } | grep name ; done Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-mv26c\" Server name: \"httpenv-cfb65dd68-mv26c\" Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-mv26c\" Server name: \"httpenv-cfb65dd68-lkz6f\" Server name: \"httpenv-cfb65dd68-mv26c","title":"Test HTTP Load Balancing"},{"location":"how-to/k8s-tips-tricks/#kill-pod","text":"Do not wait grace period for signal propagation. # Delete a pod with minimal delay $ kubectl delete pod foo --now # Force delete a pod on a dead node $ kubectl delete pod foo --force $ kubectl delete pod foo --grace-period = -1 # Period of time in seconds given to the resource to # terminate gracefully. Ignored if negative. # Set to 1 for immediate shutdown. Can only be set # to 0 when --force is true (force deletion).","title":"Kill POD"},{"location":"how-to/k8s-tips-tricks/#deployment-replicaset-pod-relations","text":"Deployment always create a replicaSet A replicaSet always create at least one POD $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE pingpong 1 /1 1 1 4m25s $ kubectl get rs NAME DESIRED CURRENT READY AGE pingpong-6777998c97 1 1 1 4m29s $ kubectl get pods NAME READY STATUS RESTARTS AGE pingpong-6777998c97-l8dmk 1 /1 Running 0 4m34s","title":"Deployment / Replicaset / POD relations"},{"location":"how-to/k8s-tips-tricks/#scale-deployment","text":"","title":"Scale deployment"},{"location":"how-to/k8s-tips-tricks/#instantiate-scale-up-process","text":"$ kubectl scale deployment nginx-basics --replicas = 3 deployment.apps/nginx-basics scaled","title":"Instantiate scale up process"},{"location":"how-to/k8s-tips-tricks/#check-k8s-status","text":"$ kubectl get deployments.apps nginx-basics NAME READY UP-TO-DATE AVAILABLE AGE nginx-basics 3 /3 3 3 19h $ kubectl get pods --selector = app = webui NAME READY STATUS RESTARTS AGE nginx-basics-c48d789fd-kzcbt 1 /1 Running 1 19h nginx-basics-c48d789fd-vbrmv 1 /1 Running 0 3m39s nginx-basics-c48d789fd-vx9t5 1 /1 Running 0 3m39s $ kubectl describe deployments.apps nginx-basics Name: nginx-basics Namespace: default CreationTimestamp: Tue, 23 Jun 2020 15 :44:44 +0200 Labels: <none> Annotations: deployment.kubernetes.io/revision: 1 kubectl.kubernetes.io/last-applied-configuration: ... Selector: app = webui Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25 % max unavailable, 25 % max surge Pod Template: Labels: app = webui Containers: nginx: Image: titom73/nginx Port: 80 /TCP Host Port: 0 /TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: <none> NewReplicaSet: nginx-basics-c48d789fd ( 3 /3 replicas created ) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 4m5s deployment-controller Scaled up replica set nginx-basics-c48d789fd to 3","title":"Check K8S status"},{"location":"how-to/k8s-tips-tricks/#check-with-server-requests","text":"$ while sleep 0 .3 ; do curl -s test.lab.as73.inetsix.net/basics | grep name ; done Server name: nginx-basics-c48d789fd-kzcbt Server name: nginx-basics-c48d789fd-vbrmv Server name: nginx-basics-c48d789fd-vx9t5 Server name: nginx-basics-c48d789fd-kzcbt Server name: nginx-basics-c48d789fd-vbrmv","title":"Check with Server requests"},{"location":"how-to/k8s-tips-tricks/#tricks","text":"","title":"Tricks"},{"location":"how-to/k8s-tips-tricks/#dns-configuration","text":"$ microk8s enable dns $ microk8s kubectl -n kube-system edit configmap/coredns # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready log . { class error } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 8 .8.8.8 8 .8.4.4 cache 30 loop reload loadbalance } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | [ ... output truncated ... ]","title":"DNS configuration"},{"location":"how-to/k8s-tips-tricks/#issue-with-dns-resolution-on-alpine","text":"k8s-journey/manifests on \ue0a0 master [?] at \u2638\ufe0f microk8s \u279c kubectl describe pod pingpong-8558fd45c7-pqc7p Name: pingpong-8558fd45c7-pqc7p Namespace: default Priority: 0 Node: k8s-node01/10.73.1.248 Start Time: Mon, 22 Jun 2020 22:50:15 +0200 Labels: app=pingpong pod-template-hash=8558fd45c7 [... output truncated ...] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m38s default-scheduler Successfully assigned default/pingpong-8558fd45c7-pqc7p to k8s-node01 Normal Pulling 4m22s (x3 over 4m37s) kubelet, k8s-node01 Pulling image \"alpine\" Normal Pulled 4m22s (x3 over 4m36s) kubelet, k8s-node01 Successfully pulled image \"alpine\" Normal Created 4m21s (x3 over 4m36s) kubelet, k8s-node01 Created container alpine Normal Started 4m21s (x3 over 4m36s) kubelet, k8s-node01 Started container alpine Warning BackOff 4m9s (x4 over 4m34s) kubelet, k8s-node01 Back-off restarting failed container Warning DNSConfigForming 3m55s (x9 over 4m38s) kubelet, k8s-node01 Search Line limits were exceeded, some search paths have been omitted, the applied search line is: default.svc.cluster.local svc.cluster.local cluster.local lab.as73.inetsix.net as73047inetsix.net inetsix.net Reason is search name in DNS cannot exceed 6 dns domains : const ( // Limits on various DNS parameters. These are derived from // restrictions in Linux libc name resolution handling. // Max number of DNS name servers. MaxDNSNameservers = 3 // Max number of domains in search path. MaxDNSSearchPaths = 6 // Max number of characters in search path. MaxDNSSearchListChars = 256 ) Discussion on Stackoverflow","title":"Issue with DNS resolution on Alpine"},{"location":"how-to/virtual-env-python/","text":"VirtualEnv Wrapper \u00b6 VirtualEnvWrapper allows user to create python virtual-environment and use easy command to create / use / modify / delete Installation \u00b6 Use python-pip to install module: $ pip install virtualenvwrapper To test in your shell: Create folder to store virtual-environments: $ mkdir ~/.virtual-envs $ mkdir ~/Scripting/ Load vars to your shell $ export WORKON_HOME = ~/.virtual-envs $ export PROJECT_HOME = ~/Scripting/ $ source /usr/local/bin/virtualenvwrapper.sh To make a permanent configuration: $ vim ~/.zshrc export WORKON_HOME = ~/.virtual-envs export PROJECT_HOME = ~/Scripting/ source /usr/local/bin/virtualenvwrapper.sh Commands \u00b6 Create virtual environment $ mkvirtualenv my_venv List existing environments $ lsvirtualenv Load existing environment $ workon my_env Create project with a new venv $ mkproject myproject Project is created under PROJECT_HOME with a dedicated folder and a venv with project\u2019s name Leave a venv deactivate Delete a venv $ rmvirtualenv my_venv Resources \u00b6 List of commands Hooks and customisation","title":"VirtualEnv Wrapper"},{"location":"how-to/virtual-env-python/#virtualenv-wrapper","text":"VirtualEnvWrapper allows user to create python virtual-environment and use easy command to create / use / modify / delete","title":"VirtualEnv Wrapper"},{"location":"how-to/virtual-env-python/#installation","text":"Use python-pip to install module: $ pip install virtualenvwrapper To test in your shell: Create folder to store virtual-environments: $ mkdir ~/.virtual-envs $ mkdir ~/Scripting/ Load vars to your shell $ export WORKON_HOME = ~/.virtual-envs $ export PROJECT_HOME = ~/Scripting/ $ source /usr/local/bin/virtualenvwrapper.sh To make a permanent configuration: $ vim ~/.zshrc export WORKON_HOME = ~/.virtual-envs export PROJECT_HOME = ~/Scripting/ source /usr/local/bin/virtualenvwrapper.sh","title":"Installation"},{"location":"how-to/virtual-env-python/#commands","text":"Create virtual environment $ mkvirtualenv my_venv List existing environments $ lsvirtualenv Load existing environment $ workon my_env Create project with a new venv $ mkproject myproject Project is created under PROJECT_HOME with a dedicated folder and a venv with project\u2019s name Leave a venv deactivate Delete a venv $ rmvirtualenv my_venv","title":"Commands"},{"location":"how-to/virtual-env-python/#resources","text":"List of commands Hooks and customisation","title":"Resources"}]}